<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring boot之Actuator理解]]></title>
    <url>%2Fblog%2F2018%2F08%2F27%2FSpring-boot%E4%B9%8BActuator%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Environment Spring boot 2.0.4 Jdk1.8 Preface今天生产上的日志文件把磁盘撑爆了,非常悲剧.既然出了问题,就得找到原因以及解决方案.(一劳永逸) 通过梳理,识别出以下几个问题: 代码中的日志没有行至有效的规范.debug和info没有明确的规范. 有些debug日志有助于生产上排查错误,需要动态切换日志级别的能力. 针对日志规范问题,每个人的见解不一样,我自己的梳理在《Java日志规范看法》中. 根据第二个问题,发现spring boot actuator提供了这个能力,这就促使我去研究一番. Spring Boot Actuator○ Spring Boot includes a number of additional features to help you monitor and manage your application when you push it to production. You can choose to manage and monitor your application by using HTTP endpoints or with JMX. Auditing, health, and metrics gathering can also be automatically applied to your application. ☆ Spring boot 提供了以HTTP或JMX管理和监控应用程序.适用于应用程序的审计、健康情况、度量收集. 依赖包maven项目 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 提供的端点 详细用法参考Usage. 下表提供了16个请求端点,HTTP和JMX都可以使用.另外4个端点在使用web时可以用. 请求前缀为/actuators 例如:以http方式,监测应用程序健康情况:/actuators/health 12$ curl http://localhost:[port]/actuator/health&#123;&quot;status&quot;:&quot;UP&quot;&#125; 列表: ID Description Enabled by default auditevents Exposes audit events information for the current application. Yes beans Displays a complete list of all the Spring beans in your application. Yes conditions Shows the conditions that were evaluated on configuration and auto-configuration classes and the reasons why they did or did not match. Yes configprops Displays a collated list of all @ConfigurationProperties. Yes env Exposes properties from Spring’s ConfigurableEnvironment. Yes flyway Shows any Flyway database migrations that have been applied. Yes health Shows application health information. Yes httptrace Displays HTTP trace information (by default, the last 100 HTTP request-response exchanges). Yes info Displays arbitrary application info. Yes loggers Shows and modifies the configuration of loggers in the application. Yes liquibase Shows any Liquibase database migrations that have been applied. Yes metrics Shows ‘metrics’ information for the current application. Yes mappings Displays a collated list of all @RequestMapping paths. Yes scheduledtasks Displays the scheduled tasks in your application. Yes sessions Allows retrieval and deletion of user sessions from a Spring Session-backed session store. Not available when using Spring Session’s support for reactive web applications. Yes shutdown Lets the application be gracefully shutdown. No threaddump Performs a thread dump. Yes ○ If your application is a web application (Spring MVC, Spring WebFlux, or Jersey),you can use the following additional endpoints. ID Description Enabled by default heapdump Returns a GZip compressed hprof heap dump file. Yes jolokia Exposes JMX beans over HTTP (when Jolokia is on the classpath, not available for WebFlux). Yes logfile Returns the contents of the logfile (if logging.file or logging.path properties have been set). Supports the use of the HTTP Range header to retrieve part of the log file’s content. Yes prometheus Exposes metrics in a format that can be scraped by a Prometheus server. Yes 开启端点 actuator中提供的20个端点,默认级别开关级别如下: ID JMX Web auditevents Yes No beans Yes No conditions Yes No configprops Yes No env Yes No flyway Yes No health Yes Yes heapdump N/A No httptrace Yes No info Yes Yes jolokia N/A No logfile N/A No loggers Yes No liquibase Yes No metrics Yes No mappings Yes No prometheus N/A No scheduledtasks Yes No sessions Yes No shutdown Yes No threaddump Yes No JMX 除prometheus、logfile、jolokia、heapdump没有提供外,其余端点默认开启. Web 默认只开启health、info端点. 正确打开姿势: JMX 1234567# 打开 management.endpoints.jmx.exposure.include# 关闭 management.endpoints.jmx.exposure.exclude# 如关闭mappings、shutdown端点.management.endpoints.jmx.exposure.exclude=mappings,shutdown# 如关闭所有端点.management.endpoints.jmx.exposure.exclude=*# YAML中*有特殊含义,需要用&quot;*&quot; Web 1234567# 打开 management.endpoints.web.exposure.include# 关闭 management.endpoints.web.exposure.exclude# 如打开env、mappings端点management.endpoints.web.exposure.include=env,mappings# 如打开所有端点management.endpoints.web.exposure.include=*# YAML中*有特殊含义,需要用&quot;*&quot; 端点详细用法请求前缀为/actuators.具体用法参考文档Usage. 配置端点缓存○ Endpoints automatically cache responses to read operations that do not take any parameters. ☆ 官方说是默认缓存读取操作的无参端点返回值.至于具体时间只能看源码了. 对应的源码类为:EndpointAutoConfiguration、EndpointIdTimeToLivePropertyFunction 正确配置姿势如下: 123# The prefix management.endpoint.&lt;name&gt; is used to uniquely identify the endpoint that is being configured.# 如配置bean端点返回值缓存时间为10秒management.endpoint.beans.cache.time-to-live=10s 自定义端点路径 base-path 默认值是:/actuator path-mapping 端点映射路径,默认是官方提供的20个端点名称. 完整请求路径为:[base-path]+[path-mapping] 123# 原health端点为:/actuator/health,现在改为:/manager/healthcheckmanagement.endpoints.web.base-path=/managermanagement.endpoints.web.path-mapping.health=healthcheck 自定义端点○ If you add a @Bean annotated with @Endpoint, any methods annotated with @ReadOperation, @WriteOperation, or @DeleteOperation are automatically exposed over JMX and, in a web application, over HTTP as well. Endpoints can be exposed over HTTP using Jersey, Spring MVC, or Spring WebFlux. ○ You can also write technology-specific endpoints by using @JmxEndpoint or @WebEndpoint. These endpoints are restricted to their respective technologies. For example, @WebEndpoint is exposed only over HTTP and not over JMX. ○ You can write technology-specific extensions by using @EndpointWebExtension and @EndpointJmxExtension. These annotations let you provide technology-specific operations to augment an existing endpoint. ○ Finally, if you need access to web-framework-specific functionality, you can implement Servlet or Spring @Controller and @RestController endpoints at the cost of them not being available over JMX or when using a different web framework. ☆ actuator可以提供JMX和HTTP两种方式,所以也提供对应实现的方式. @Endpoint针对JMX和HTTP. @JmxEndpoint或@EndpointJmxExtension只针对JMX. @WebEndpoint或@EndpointWebExtension只针对HTTP. @ReadOperation, @WriteOperation, @DeleteOperation 这三个用于指定请求方式. Operation HTTP method @ReadOperation GET @WriteOperation POST @DeleteOperation DELETE ☆ 使用actuator的自定义端点有特别的意义吗?目前看,针对HTTP的方式,与平常我自己暴露端点也没区别.但针对JMX监控的话,这就是它优势之处了. 另外,个人认为没有特殊要求,使用@Endpoint更好,既提供了JMX监控端点,也同时提供了HTTP监控端点. 如何使用○ To allow the input to be mapped to the operation method’s parameters, Java code implementing an endpoint should be compiled with -parameters, and Kotlin code implementing an endpoint should be compiled with -java-parameters. This will happen automatically if you are using Spring Boot’s Gradle plugin or if you are using Maven and spring-boot-starter-parent. ☆ 在使用@ReadOperation等时,默认是按照参数名匹配入参,如果需要参数数量自动匹配,需要在spring boot时添加-parameters. 新增端点 使用@Endpoint、@ReadOperation, @WriteOperation, @DeleteOperation 例子: 1234567891011121314151617181920212223242526272829@Component@Endpoint(id = &quot;custom-health&quot;)public class CustomHealthEndpoint &#123; @ReadOperation public String health() &#123; return &quot;health&quot;; &#125; @ReadOperation public String health2(@Selector String name) &#123; return &quot;custom-end-point get parameter: &quot; + name; &#125; @ReadOperation public String health3(@Selector String name, @Selector String name2) &#123; return &quot;custom-end-point get parameter1: &quot; + name +&quot;,parameter2: &quot; + name2; &#125; @WriteOperation public String writeOperation(@Selector String name) &#123; return &quot;custom-end-point post&quot;; &#125; @DeleteOperation public String deleteOperation(@Selector String name) &#123; return &quot;custom-end-point delete&quot;; &#125;&#125; 启动时,使用-parameters参数. 1234567# 调用health2方法 http://localhost:[port]/actuator/custom-health/&#123;anystring&#125;$ curl http://localhost:[port]/actuator/custom-health/hellocustom-end-point get parameter: hello# 调用health3方法,参数名必须相同,&#123;angstring&#125;用任意字符串替换就行.http://localhost:[port]/actuator/custom-health/&#123;anystring&#125;/&#123;anystring&#125;$ curl http://localhost:[port]/actuator/custom-health/hello/worldcustom-end-point get parameter1: hello,parameter2: world 启动时,不使用-parameters参数. 123456789101112131415# 调用health无参方法$ curl http://localhost:[port]/actuator/custom-healthhealth# 调用health2方法$ curl http://localhost:[port]/actuator/custom-health/name&#123;&quot;timestamp&quot;:&quot;2018-08-27T11:04:06.709+0000&quot;,&quot;status&quot;:400,&quot;error&quot;:&quot;Bad Request&quot;,&quot;message&quot;:&quot;Missing parameters: name&quot;,&quot;path&quot;:&quot;/actuator/custom-health/p1&quot;&#125;只能通过以下方式才能请求到health2方法,参数名必须相同.&#123;angstring&#125;用任意字符串替换就行.$ curl http://localhost:[port]/actuator/custom-health/&#123;anystring&#125;?name=hellocustom-end-point get parameter: hello# 调用health3方法,参数名必须相同,&#123;angstring&#125;用任意字符串替换就行.$ curl http://localhost:[port]/actuator/custom-health/&#123;anystring&#125;/&#123;anystring&#125;?name=hello&amp;name=worldcustom-end-point get parameter1: hello,parameter2: world ☆ 总结:actuator中端点的多参数请求方式,不按照参数名匹配.所以需要在启动时添加-parameters参数. 使用WebEndpointExtension或@EndpointJmxExtension 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Endpoint(id = &quot;myhealth&quot;)public class MyHealthEndpoint &#123; @ReadOperation public String health() &#123; return &quot;health&quot;; &#125;&#125;@EndpointWebExtension(endpoint = MyHealthEndpoint.class)public class MyHealthWebEndpointExtension &#123; private final MyHealthEndpoint delegate; public MyHealthWebEndpointExtension(MyHealthEndpoint delegate) &#123; this.delegate = delegate; &#125; @ReadOperation public WebEndpointResponse&lt;String&gt; getHealth() &#123; return new WebEndpointResponse&lt;&gt;(&quot;health&quot;, 200); &#125;&#125;@Configurationpublic class ActuatorConfiguration &#123; @Bean @ConditionalOnMissingBean @ConditionalOnEnabledEndpoint public MyHealthEndpoint myHealthEndpoint() &#123; return new MyHealthEndpoint(); &#125; @Bean @ConditionalOnMissingBean @ConditionalOnEnabledEndpoint @ConditionalOnBean(&#123;MyHealthEndpoint.class&#125;) public MyHealthWebEndpointExtension myHealthWebEndpointExtension( MyHealthEndpoint delegate) &#123; return new MyHealthWebEndpointExtension(delegate); &#125;&#125;# application.ymlmanagement: endpoints: myhealth: enabled: true 覆盖原端点目前覆盖原端点只能通过重载的方式.我这里测试了health端点的覆盖. 12345678910@Componentpublic class MyHealthIndicator implements HealthIndicator &#123; @Override public Health health() &#123; return Health.down().build(); &#125;&#125;# 再次请求Health端点,返回值就为&#123;&quot;status&quot;:&quot;DOWN&quot;&#125; 遗留问题端点默认缓存的默认时间是多少?官网还有更多关于监控文章待学习.参考spring-boot-2.0.4-docCustom Endpoint in Spring Boot ActuatorHow to make the @Endpoint(id = “health”) working in Spring Boot 2.0?]]></content>
      <categories>
        <category>Spring boot</category>
      </categories>
      <tags>
        <tag>Spring boot 监控</tag>
        <tag>Spring boot Actuator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes的DNS理解]]></title>
    <url>%2Fblog%2F2018%2F08%2F23%2Fkubernetes%E7%9A%84DNS%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Foreward对于k8s的DNS理解还是有些模糊,这里梳理阅读相关文章后的理解. Using CoreDNS for Service Discovery 在 Kubernetes 中配置私有 DNS 和上游域名服务器 DNS for Services and Pods Customizing DNS Service Kubernetes提供的DNS服务DNS是kubernetes内置的Pod服务.包含三个容器: kubedns 监测kubernetes的master节点对Services和Endpoints的变化,并且保留在内存中,服务DNS查询. dnsmasq 缓存DNS,提高查询效率. sidecars 为dnsmasq和kubedns提供健康检查的端点. Kube-DNS和CoreDNSkubernetes提供了两种DNS服务 kube-dns CoreDNS 从v1.11版本开始,CoreDNS已经是GA版本了,且已经作为Kubernetes的DNS服务.(CoreDNS已经作为CNCF的独立项目) kube-dns是在1.9版本前使用. 在v1.11版本之后,(不建议)如果还想继续使用kube-dns,则在初始化集群是配置以下参数 1$ kubeadm init --feature-gates=CoreDNS=false 配置kube-dns的存根域和上游DNS服务器这里有两个概念:stub domains 和upstream nameservers.这里我翻译为存根域和上游DNS服务器. upstreamNameservers,会覆盖node节点上的/etc/resolv.conf文件,且最多配置三个upstream nameservers. 例子: 12345678910apiVersion: v1kind: ConfigMapmetadata: name: kube-dns namespace: kube-systemdata: stubDomains: | &#123;&quot;acme.local&quot;: [&quot;1.2.3.4&quot;]&#125; upstreamNameservers: | [&quot;8.8.8.8&quot;, &quot;8.8.4.4&quot;] DNS请求如果后缀有acme.local,则返回DNS Server的地址1.2.3.4. Domain name Server answering the query kubernetes.default.svc.cluster.local kube-dns foo.acme.local custom DNS (1.2.3.4) widget.com upstream DNS (one of 8.8.8.8, 8.8.4.4) Pod设置dnsPolicy对DNS查询的影响当在pod中设置的dnsPolicy为default 和None,则自定义的stub domain和upstream nameservers不会生效. 当dnsPolicy为ClusterFirst后 未配置了存根域和upstream 如果咩有匹配的domain后缀,如www.kubernetes.io则去查找upstream nameserver. 配置自定义存根域和upstream 首先查找kube-dns的DNS cache. 再查找自定义的stub domain,即图中的custom DNS. 最后查找upstream DNS. 配置CoreDNS的存根域和上游DNS服务器CoreDNS提供链条插件式扩展,非常灵活.CoreDNS安装后默认包含了30个插件.CoreDNS的功能可以由一个或多个插件组成.只要会go语言,以及指导DNS工作原理就可以开发插件. CoreDNS的配置文件Corefile.且语法规则如下: 12345678coredns.io &#123; file coredns.io.signed &#123; transfer to * 185.49.140.62 &#125; prometheus errors log&#125; 详细信息可浏览官网CoreDNS. 在v1.10版本后,kubeadm支持自动转换ConfigMap为Corefile. Example: kubedns使用以下配置.stubDomain存根域及upstream上游域. 123456789apiVersion: v1data: federations: | &#123;&quot;foo&quot; : &quot;foo.feddomain.com&quot;&#125; stubDomains: | &#123;&quot;abc.com&quot; : [&quot;1.2.3.4&quot;], &quot;my.cluster.local&quot; : [&quot;2.3.4.5&quot;]&#125; upstreamNameservers: | [&quot;8.8.8.8&quot;, &quot;8.8.4.4&quot;]kind: ConfigMap 等价的Corefile配置文件为: For federations: 123federation cluster.local &#123; foo foo.feddomain.com &#125; For stubDomains: 12345678910abc.com:53 &#123; errors cache 30 proxy . 1.2.3.4&#125;my.cluster.local:53 &#123; errors cache 30 proxy . 2.3.4.5&#125; 完整配置如下:DNS使用UDP协议,且端口为53 12345678910111213141516171819202122232425.:53 &#123; errors health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; upstream 8.8.8.8 8.8.4.4 pods insecure fallthrough in-addr.arpa ip6.arpa &#125; federation cluster.local &#123; foo foo.feddomain.com &#125; prometheus :9153 proxy . 8.8.8.8 8.8.4.4 cache 30 &#125; abc.com:53 &#123; errors cache 30 proxy . 1.2.3.4 &#125; my.cluster.local:53 &#123; errors cache 30 proxy . 2.3.4.5 &#125; DNS中的记录生成规则DNS包含A记录和SRV记录.A记录就是ip和域名的映射,SRV记录是端口映射. ServiceService分为Headless和非Headless.(Headless Service:.spec.clusterIP设置为None) Service创建之后,默认会生成一条DNS映射的A记录,格式为:[.metadata.name].[namespace].svc.cluster.local. 还会生成一条DNS映射的SRV(端口)记录,格式为:_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local. 对非Headless的Service,端口的DNS映射就是:[.metadata.name].[namespace].svc.cluster.local. 对于Headless的Service,目前还不是特别明白.暂且先将原文描述贴下来.For a headless service, this resolves to multiple answers, one for each pod that is backing the service, and contains the port number and the domain name of the pod of the form auto-generated-name.my-svc.my-namespace.svc.cluster.local. Pod创建Pod会生成一条DNS的A记录:pod-ip-address.my-namespace.pod.cluster.local. 在集群中查找Pod,可以通过这种格式[.metadata.name].[.spec.subdomain].[namespace].svc.cluster.local查找. Pod的DNS规则 设置字段:.spec.dsnPolicy.有四种规则: Default 虽然名字是Default,但是不是默认规则. The Pod inherits the name resolution configuration from the node that the pods run on ClusterFirst 集群规则优先,如果没有查询到,则去上游域名服务器查询.集群DNS服务和上游DNS服务都可以配置. ClusterFirstWithHostNet For Pods running with hostNetwork, you should explicitly set its DNS policy None 忽略在kubernetes环境DNS配置,并使用自定义的DNS配置..spec.dsnConfig 如何手动设置Pod中的DNS解析配置需要在集群中开启功能支持,--feature-gates=CustomPodDNS=true. 例如: 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: namespace: default name: dns-examplespec: containers: - name: test image: nginx dnsPolicy: &quot;None&quot; dnsConfig: nameservers: - 1.2.3.4 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: &quot;2&quot; - name: edns0 运行后,会在Pod中的/etc/resolv.conf文件中生成以下内容: 123nameserver 1.2.3.4search ns1.svc.cluster.local my.dns.search.suffixoptions ndots:2 edns0 自定义DNS服务]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS for Services and Pods翻译]]></title>
    <url>%2Fblog%2F2018%2F08%2F22%2FDNS-for-Services-and-Pods%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[原文链接DNS for Services and Pods这篇文章是kubernetes关于DNS的概述. Introduction Services Pods IntroductionKubernetes DNS 在集群中调度一个DNS Pod和Service,并配置kubelets去告诉独立的容器使用DNS Service’s IP去解析DNS名称. What things get DNS names?在集群中的每个service都会被分配一个DNS名称.默认情况下,客户端发起的Pod的DNS搜索包括Pod的namespace和集群默认的域名.这里有个例子说明: 假设一个Service名称为foo,在kubernetes中的namespace为bar.一个运行在namespace为bar的pod,通过简便的DNS查询到名称为foo的Service.一个运行在namespace为quux的pod,可以在DNS中通过搜索名称为foo.bar,并查到这个容器. 下面的章节详细的说明了支持的record类型以及支持的布局设计. ServicesA Record正常(not headless)Services会被分配一个这种格式的DNS A 记录my-svc.my-namespace.svc.cluster.local.这个解析集群的Service Ip. Headless(没有cluster ip)Services会被分配一个这种格式的DNS A记录my-svc.my-namespace.svc.cluster.local.和正常的Services不同的是,通过解析Pod中的一组Ip. SRV RecordSRV记录是在正常或Headless Service创建时指定端口.每个指定的端口,SRV记录的格式是: _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local.对于headless serviec来说,这个解析会得到多个结果,pod中的容器端口号和子域名格式为:auto-generated-name.my-svc.my-namespace.svc.cluster.local. PodsA RecordPods会被分配这种格式的一条DNS A记录:pod-ip-address.my-namespace.pod.cluster.local 例如:一个pod的ip是1.2.3.4,namespace为default,DNS为cluster.local的DNS记录为:1-2-3-4.default.pod.cluster.local Pod’s hostname and subdomain fields创建pod时,它的hostname默认为Pod中的metadata.name的值. Pod的spec有个hostname字段选项,用来指定pod的hostname.而.spec.hostname优先级高于.metadata.name.例如:给一个pod的hostname设置为”my-host”,那么这个pod的名称就是”my-host”. Pod的spec有个subdomain字段选项,用来指定pod的子域名.例如:一个pod的hostname设置为”foo”,subdomain设置为”bar”,namespace为”my-namespace”,则它的查询名称就为:foo.bar.my-namespace.svc.cluster.local 例子: 1234567891011121314151617181920212223242526272829303132333435363738394041424344apiVersion: v1kind: Servicemetadata: name: default-subdomainspec: selector: name: busybox clusterIP: None ports: - name: foo # Actually, no port is needed. port: 1234 targetPort: 1234---apiVersion: v1kind: Podmetadata: name: busybox1 labels: name: busyboxspec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox command: - sleep - &quot;3600&quot; name: busybox---apiVersion: v1kind: Podmetadata: name: busybox2 labels: name: busyboxspec: hostname: busybox-2 subdomain: default-subdomain containers: - image: busybox command: - sleep - &quot;3600&quot; name: busybox 如果存在一个headless service和一个pod在同一个namespace中,并且headless service的名称和pod中的subdomain名称相同,kubernetes的DNS服务一样查询到这个Service.例如:上面配置中,一个Pod的hostname是”busybox-1”,subdomain是default-subdomain,而headless service命名为”default-subdomain”.那么这个DNS为busybox-1.default-subdomain.my-namespace.svc.cluster.local]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>DNS for Services and Pods翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes常用命令]]></title>
    <url>%2Fblog%2F2018%2F08%2F21%2Fkubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[控制器Deployment123456789101112131415161718192021# nginx-deployment.yaml$ vim nginx-deployment.yamlapiVersion: apps/v1 # 1.11版本之后kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 3 # 3个副本 selector: matchLabels: app: nginx # 与template.metadata.labels一致 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 创建Deployments record参数设置为true,在Deployment revision时方便命令记录,以及在describe时能够看到Annotations中指令记录 12$ kubectl create -f nginx-deployment.yaml --recorddeployment &quot;nginx-deployment&quot; created 查看发布历史记录 rollout 字面是上线意思,我理解为发布. 替换metadata.name,即上面文件中的nginx-deployment名称. 12345678910# kubectl rollout history deployment/[metadata.name]$ kubectl rollout history deployment/nginx-deployment# 如果在创建Deployments时没有使用--record,没有命令记录.deployments &quot;nginx-deployment&quot;REVISION CHANGE-CAUSE1 &lt;none&gt;# 如果在创建时指定--record,会有命令记录.deployments &quot;nginx-deployment&quot;REVISION CHANGE-CAUSE1 kubectl create --filename=nginx-deployment.yaml --record=true 查看历史版本的具体信息 1234567891011121314$ kubectl rollout history deployment/nginx-deployment --revision=1deployments &quot;nginx-deployment&quot; with revision #1Pod Template: Labels: app=nginx pod-template-hash=2777190766 Annotations: kubernetes.io/change-cause=kubectl create --filename=nginx-deployment.yaml --record=true Containers: nginx: Image: nginx Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; 查看Deployments12345$ kubectl get deployment [metadata.name](可选)# 不指定deployment的名字将查询全部的deployments.如下:NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEhello-node-deployment 1 1 1 1 17mnginx-deployment 1 1 1 1 4m 查看RS(ReplicaSet)123$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-966857787 1 1 1 5m 更新Deployments(自动rollout)123456# 方式一,修改文件中的spec.template.spec.containers[0].image值kubectl edit deployment/[metadata.name]$ kubectl edit deployment/nginx-deployment# 方式二 kubectl set image deployment [metadata.name] [spec.spec.containers.name]=镜像名称$ kubectl set image deployment nginx-deployment nginx=nginx:1.7.9 查看发布状态12345678910# kubectl rollout status deployment/[metadata.name]$ kubectl rollout status deployment/nginx-deploymentWaiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 old replicas are pending termination...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 old replicas are pending termination...deployment &quot;nginx-deployment&quot; successfully rolled out 查看deployment详细信息 关注Events,记录了发布的过程.实际是创建了一个新的ReplicaSet-&gt;nginx-deployment-6ccc5f4cbb,然后逐渐下原来的ReplicaSet-&gt;nginx-deployment-966857787的Pod. 12345678910111213141516171819202122232425262728293031323334353637383940# kubectl describe deployment/[metadata.name]$ kubectl describe deployment/nginx-deploymentName: nginx-deploymentNamespace: defaultCreationTimestamp: Tue, 21 Aug 2018 15:18:18 +0800Labels: app=nginxAnnotations: deployment.kubernetes.io/revision=2 kubernetes.io/change-cause=kubectl create --filename=nginx-deployment.yaml --record=trueSelector: app=nginxReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surgePod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailableOldReplicaSets: &lt;none&gt;NewReplicaSet: nginx-deployment-6ccc5f4cbb (3/3 replicas created)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 4m deployment-controller Scaled up replica set nginx-deployment-966857787 to 3 Normal ScalingReplicaSet 3m deployment-controller Scaled up replica set nginx-deployment-6ccc5f4cbb to 1 Normal ScalingReplicaSet 3m deployment-controller Scaled down replica set nginx-deployment-966857787 to 2 Normal ScalingReplicaSet 3m deployment-controller Scaled up replica set nginx-deployment-6ccc5f4cbb to 2 Normal ScalingReplicaSet 3m deployment-controller Scaled down replica set nginx-deployment-966857787 to 1 Normal ScalingReplicaSet 3m deployment-controller Scaled up replica set nginx-deployment-6ccc5f4cbb to 3 Normal ScalingReplicaSet 3m deployment-controller Scaled down replica set nginx-deployment-966857787 to 0 版本回退 回滚之后,revision对应记录就会消失 undo是回滚到上一个版本的操作. 假设有三个版本:nginx:1.4.7, nginx:1.7.9, nginx:1.9.7, nginx:1.10.3,当前版本为nginx:1.10.3. 1、假如指定to-revision回滚到1.7.9版本,再执行undo(不指定to-revision),则恢复到1.10.3版本. 2、第一次执行undo(不指定to-revision),回滚到1.9.7版本,再次执行undo(不指定to-revision),则恢复到1.10.3版本. 3、第一次指定to-revision回滚到1.7.9,第二次指定to-revision回滚到1.9.7,第三次指定to-revision回滚到1.4.7,第四次指定undo(不指定to-revison),则是回滚到1.9.7. 假设有三个版本:nginx:1.7.9,nginx:1.9.7,nginx:1.10.3,当前版本为nginx:1.10.3. 1234567# 查看发布历史记录$ kubectl rollout history deployment/nginx-deployment deployments &quot;nginx-deployment&quot;REVISION CHANGE-CAUSE1 kubectl create --filename=nginx-deployment.yaml --record=true2 kubectl create --filename=nginx-deployment.yaml --record=true3 kubectl create --filename=nginx-deployment.yaml --record=true 第一次执行undo回退到前一个版本,即nginx:1.9.7.如果第二次再执行,又会回滚到原版本,即nginx:1.10.3. 123456789# kubectl rollout undo deployment/[metatdata.name]$ kubectl rollout undo deployment/nginx-deploymentdeployment.extensions/nginx-deployment# 这里查看下发布历史记录,发现revision为3的记录消失了.deployments &quot;nginx-deployment&quot;REVISION CHANGE-CAUSE1 kubectl create --filename=nginx-deployment.yaml --record=true2 kubectl create --filename=nginx-deployment.yaml --record=true4 kubectl create --filename=nginx-deployment.yaml --record=true 指定to-revision回退到指定历史 1234# kubectl rollout undo deployment/[metadata.name] --to-revision=[number]$ kubectl rollout undo deployment/nginx-deployment --to-revision=2deployment.extensions/nginx-deployment# 这里如果执行undo但不指定--to-revision,则恢复导原来版本. Deployment扩容指定 扩容/缩容 副本数量 123456789# kubectl scale deployment/[metadata.name] --replicas=[number]$ kubectl scale deployment/nginx-deployment --replicas=6deployment.extensions/nginx-deployment scaled# 查看扩容状态$ kubectl rollout status deployment/nginx-deploymentWaiting for deployment &quot;nginx-deployment&quot; rollout to finish: 3 of 6 updated replicas are available...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 4 of 6 updated replicas are available...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 5 of 6 updated replicas are available...deployment &quot;nginx-deployment&quot; successfully rolled out 当集群启用horizontal pod autoscaling后,可以根据CPU利用率,在范围内扩容或缩容.(todo还不知道怎么做) 12$ $ kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80deployment &quot;nginx-deployment&quot; autoscaled 设置Deployments发布历史记录在nginx-deployment.yaml中设置spec.revisionHistoryLimits属性,默认是保留全部历史记录.可以不用去理会. 设置Deployments发布策略spec.strategy 指定新的Pod替换旧的Pod的策略。 spec.strategy.type 可以是Recreate或者是 RollingUpdate。RollingUpdate是默认值。 Recreate 指在创建出新的Pod之前会杀掉已经存在的Pod.(强烈建议不使用) RollingUpdate 指滚动升级.逐步一个一个交替升级.可以指定maxUnavailable 和 maxSurge 来控制 rolling update 进程。 maxUnavaiables .spec.strategy.rollingUpdate.maxUnavailable指定在升级的过程中不可用的Pod数量.默认为1,也可以设置为百分比.如设置为30%,则原来的ReplicaSet会立刻缩容到70%. maxSurge .spec.strategy.rollingUpdate.maxSurge指定在升级过程中,新老Pod的总数的最大值.如设置为30%,启动rolling update后新的ReplicatSet将会立即扩容,新老Pod的总数不能超过期望的Pod数量的130%。 Pause设置.spec.paused是可以可选配置项，boolean值。默认为false. 如果设置paused后,对Deployment中的PodTemplateSpec的修改都不会触发新的rollout。 后续需要学习如何在集群中启用了horizontal pod autoscaling参考Kubernetes apis]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gluster入门]]></title>
    <url>%2Fblog%2F2018%2F08%2F20%2FGluster%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[相关概念First, it is important to understand that GlusterFS isn’t really a filesystem in and of itself. It concatenates existing filesystems into one (or more) big chunks so that data being written into or read out of Gluster gets distributed across multiple hosts simultaneously. This means that you can use space from any host that you have available. Typically, XFS is recommended but it can be used with other filesystems as well. Most commonly EXT4 is used when XFS isn’t, but you can (and many, many people do) use another filesystem that suits you. Now that we understand that, we can define a few of the common terms used in Gluster. A trusted pool refers collectively to the hosts in a given Gluster Cluster. A node or “server” refers to any server that is part of a trusted pool. In general, this assumes all nodes are in the same trusted pool. A brick is used to refer to any device (really this means filesystem) that is being used for Gluster storage. An export refers to the mount path of the brick(s) on a given server, for example, /export/brick1 The term Global Namespace is a fancy way of saying a Gluster volume A Gluster volume is a collection of one or more bricks (of course, typically this is two or more). This is analogous to /etc/exports entries for NFS. GNFS and kNFS. GNFS is how we refer to our inline NFS server. kNFS stands for kernel NFS, or, as most people would say, just plain NFS. Most often, you will want kNFS services disabled on the Gluster nodes. Gluster NFS doesn’t take any additional configuration and works just like you would expect with NFSv3. It is possible to configure Gluster and NFS to live in harmony if you want to. Other notes: For this test, if you do not have DNS set up, you can get away with using /etc/hosts entries for the two nodes. However, when you move from this basic setup to using Gluster in production, correct DNS entries (forward and reverse) and NTP are essential. When you install the Operating System, do not format the Gluster storage disks! We will use specific settings with the mkfs command later on when we set up Gluster. If you are testing with a single disk (not recommended), make sure to carve out a free partition or two to be used by Gluster later, so that you can format or reformat at will during your testing. Firewalls are great, except when they aren’t. For storage servers, being able to operate in a trusted environment without firewalls can mean huge gains in performance, and is recommended. In case you absolutely need to set up a firewall, have a look at Setting up clientsfor information on the ports used.]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Gluster Concept</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gluster安装]]></title>
    <url>%2Fblog%2F2018%2F08%2F20%2FGluster%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境 Centos 7 Gluster 4.1 使用三台机器配置. 192.168.1.100 server1 192.168.1.101 server2 192.168.1.102 server3 前言kubernetes的持久化需要使用gluster.这里基于Centos提供的Quick-Start教程验证后的记录. InstallAdd Yum Repository添加gluster下载仓库 1$ yum install centos-release-gluster Use XFS推荐使用XFS文件系统,这里我还是使用exts4,没有重新格式化文件系统. Install Gluster And Start12$ yum install glusterfs-server$ systemctl enable glusterd &amp;&amp; systemctl start glusterd Set Firewalld防火墙配置ip级别允许规则,默认是24007端口,但是每增加bricks都会新增监听端口,所以需要配置ip级别的许可. centos7已经使用firewalld. 123# 允许指定ip.我使用三台机器做测试,每台机器要配置另外两台服务器的ip.$ firewall-cmd --permanent --add-rich-rule=&quot;rule family=&apos;ipv4&apos; source address=&apos;192.168.1.101&apos; accept&quot;$ firewall-cmd --reload Set Trusted Pool这里使用的三台机器,在server1将server2和server3添加到Trust Pool. 123# /etc/hosts需要配置映射.$ gluster peer probe server2$ gluster peer probe server3 Create a Volume在三台服务器上分别创建/bricks/brick1/gv0目录 1$ mkdir /bricks/brick1/gv0 挂载目录到gluster上.(任意节点执行命令) 123# 这里选择的replicas模式,保证数据丢失,其他模式不在这里讨论.$ gluster volume create gv0 replica 3 server1:/bricks/brick1/gv0 server2:/bricks/brick1/gv0 server3:/bricks/brick1/gv0$ gluster volume start gv0 查看volume信息 12345678910111213141516$ gluster volume infoVolume Name: gv0Type: ReplicateVolume ID: 5835a014-b598-467d-ba34-3301d6730d6fStatus: StartedSnapshot Count: 0Number of Bricks: 1 x 3 = 3Transport-type: tcpBricks:Brick1: server1:/bricks/brick1/gv0Brick2: server2:/bricks/brick1/gv0Brick3: server3:/bricks/brick1/gv0Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off Testing123456# 将server1:/gv0 挂在到/mnt目录$ mount -t glusterfs server1:/gv0 /mnt# 生成100个copy-test的文件$ for i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done# 在三台机器上查看是否都有100个文件.$ ls -lA /bricks/brick1/gv0 总结 推荐部署机器为奇数,不然会出现脑裂现象.(猜测使用了poxis算法) gluster推荐使用xfs文件系统,centos/Red Hat Enterprise Linux 7默认使用,创建系统时可能可能没有指定. 拓展 XFS vs EXT4 THE XFS FILE SYSTEM 参考gluster-Quickstart]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Gluster Install</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes Pods指定镜像仓库]]></title>
    <url>%2Fblog%2F2018%2F08%2F15%2FKubernetes-Pods%E6%8C%87%E5%AE%9A%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Foreward在创建pod时,image是从哪个仓库下载?应该如何指定公司的Regitry?最佳实践是什么?带着这些问题,记录自己的爬坑经历. 镜像建议 对每个镜像都指定明确的版本号(不要使用latest这样的版本号),减少去镜像仓库拉取的消耗.(可以学习kube的api,版本为X.Y.Z. X is the major version, Y is the minor version, and Z is the patch version) 使用镜像私仓 使用私仓时,需要配置认证信息. pod中指定私仓优先级高于node指定私仓. 在node节点中配置(推荐) 与在每个pod中使用ImagePullSecrets相比,少了重复的配置代码. 配置认证信息 docker添加私仓认证信息,可参考这里 一般使用docker时,默认都是从docker hub上拉取镜像.正常拉取是不需要认证信息的,但在上传时需要认证信息. 在每个节点上配置登录认证信息. 123# 默认docker hub的地址是:https://index.docker.io/v1$ echo [密码] | docker login --username [用户名] --password-stdin [私仓服务器地址]# 认证后,会在$HOME/.docker/下生成config.json文件.如果指定退出则会删除该文件. 认证信息优先级 {–root-dir:-/var/lib/kubelet}/config.json 默认config.json不存在 {cwd of kubelet}/config.json 指定kubelet的工作空间路径,这种方式暂不考虑. ${HOME}/.docker/config.json 默认config.json是不存在的 /.docker/config.json 默认该目录不存在 在pod中指定ImagePullSecrets创建secret12# 实际上在pull镜像时创建一个虚拟的.docker/config.json文件.$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL pod中使用imagePulSecrets123456789101112apiVersion: apps/v1 # 可以使用kubectl api-versions确定指定的版本kind: Podmetadata: name: foo namespace: awesomeappsspec: containers: - name: foo image: janedoe/awesomeapp:v1 imagePullSecrets: - name: myregistryke 以上两种方式,可以结合使用,建议每台node上都先配置认证信息,有需要再使用第二种方式. ReferenceImages]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes配置私仓</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes之pause容器]]></title>
    <url>%2Fblog%2F2018%2F08%2F12%2Fkubernetes%E4%B9%8Bpause%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Foreward之前对于docker的理解就是容器,但什么是容器呢?这个也没有去思考,借助pause容器的说明,顺便梳理下docker容器是什么?docker容器实际是利用Linux的namespace和cgroup达到容器化的目的. Two feature of Linux kernellinux有两个重要的特性,namespace和cgroup(control group).他们用于对资源进行隔离.打个比方,就像你去买商品房,每套都是用混泥土墙隔开,保证别人不会闯到你家.这个可以类比为cgroup,而cgroup就是linux对机器的物理资源(cpu,内存,磁盘io)的隔离.买完了房子,那产权当然是属于你,而不是别人(可能有小三),另外,住进去后你要用水、用电、用网络,对其他人也一样,但是每户各自结算,这就像linux环境中的user namespace、network namespace等,属于环境的隔离.由于能力有限,只能先这么去理解.这里罗列左耳朵耗子的文章. DOCKER基础技术：LINUX NAMESPACE（上） DOCKER基础技术：LINUX NAMESPACE（下） DOCKER基础技术：LINUX CGROUP AUFS 、DEVICEMAPPER对于这两个的理解,AUFS是文件管理系统,用于对文件进行分层.具体可以看耗子叔的这两篇文章.DOCKER基础技术：AUFSDOCKER基础技术：DEVICEMAPPER PauseIn Kubernetes, the pause container serves as the “parent container” for all of the containers in your pod. The pause container has two core responsibilities. First, it serves as the basis of Linux namespace sharing in the pod. And second, with PID (process ID) namespace sharing enabled, it serves as PID 1 for each pod and reaps zombie processes.对于pause的理解 参考What are Kubernetes Pods Anyway?The Almighty Pause Container]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes的pause容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes常见问题汇总]]></title>
    <url>%2Fblog%2F2018%2F08%2F11%2Fkubernetes%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[报错:/etc/kubernetes/pki/ca.crt already exists 错误描述在node节点操作kubeadm join时报出的错误. 12345678[preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found in system path[preflight] Some fatal errors occurred: [ERROR Port-10250]: Port 10250 is in use [ERROR DirAvailable--etc-kubernetes-manifests]: /etc/kubernetes/manifests is not empty [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists [ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` 解决方案: 123$ kubeadm reset# 再次执行kubeadm join$ kubeadm join &lt;ip&gt;:&lt;port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;*******&gt;]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes Q&amp;A</tag>
        <tag>kubernetes常见问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes之master搭建.md]]></title>
    <url>%2Fblog%2F2018%2F08%2F09%2Fkubernetes%E4%B9%8Bmaster%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[环境: centos7, kubernetes 1.11.2, docker-ce-17.03.2.ce ForewardKubernetes对于Master机器的配置最低的要求是2G内存和2 Core CPU.使用kubeadm来安装master cluster. Install1个master,2个nodek8s-master-1192.168.1.100k8s-node-1192.168.1.101k8s-node-2192.168.1.102 Prepare端口检查12345678910111213# Master node(s)Protocol Direction Port Range PurposeTCP Inbound 6443* Kubernetes API serverTCP Inbound 2379-2380 etcd server client APITCP Inbound 10250 Kubelet APITCP Inbound 10251 kube-schedulerTCP Inbound 10252 kube-controller-managerTCP Inbound 10255 Read-only Kubelet API# Worker node(s)Protocol Direction Port Range PurposeTCP Inbound 10250 Kubelet APITCP Inbound 10255 Read-only Kubelet APITCP Inbound 30000-32767 NodePort Services** 环境调整1234567891011121314151617# 关闭SELinux(临时)$ setenforce 0# 永久关闭SELinux$ vim /etc/selinux/configSELINUX=disabled# 关闭swap$ swapoff -a# 添加kubernetes的yum仓库,这里使用阿里云的.$ vim /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 设置hosts1234$ vim /etc/hosts192.1.1.100 k8s-master-1 192.1.1.100 k8s-node-1 192.1.1.100 k8s-node-2 Install kubeadm kubectl kubelet1$ yum install -y kubeadm kubectl kubelet Install Dockerkubernetes v1.11.2建议是用的docker版本是17.03版本,这里安装过程忽略. 下载k8s相关镜像由于在使用kubeadm init时,下载的镜像从k8s.gcr.io上下载,国内网络被墙了,这边只能曲线救国. 利用docker hub做中转(因为docker hub是在国外的). 具体操作是先在github上创建相关的Docekrfile,然后再在docker hub上创建auto build仓库.最后从自己的docker hub仓库下载镜像后,重命名为k8s.gcr.io/kube-scheduler-amd64:v1.11.2等即可. 这里有个镜像拉取脚本. (注意:v1.11版本以后,DNS服务使用coredns:1.1.3,不再使用k8s-dns相关容器,即k8s-dns-sidecar,k8s-dns-kube-dns,k8s-dns-dnsmasq) 123456789101112131415161718images=( kube-proxy-amd64:v1.11.2 kube-scheduler-amd64:v1.11.2 kube-controller-manager-amd64:v1.11.2 kube-apiserver-amd64:v1.11.2 etcd-amd64:3.2.18 pause:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.8 k8s-dns-kube-dns-amd64:1.14.8 k8s-dns-dnsmasq-nanny-amd64:1.14.8 coredns:1.1.3)for imageName in $&#123;images[@]&#125; ; do docker pull jilingjun1014/$imageName docker tag jilingjun1014/$imageName k8s.gcr.io/$imageName docker rmi jilingjun1014/$imageNamedone master初始化1234567891011121314151617181920# master初始化$ kubeadm init --kubernetes-version=v1.11.2 --pod-network-cidr=10.244.0.0/16Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 116.62.177.233:6443 --token mfv9of.3bo96kuhwiuf2sh5 --discovery-token-ca-cert-hash sha256:d04a7670ef39c41900ca142e807e96a326d6f37300076e94ce2bda4c0934ff52# 配置kubectl认证(官方推荐用非root用户,这里为了方便起见,使用root用户)$ mkdir -p ~/.kube$ cp -i /etc/kubernetes/admin.conf ~/.kube/config# 非root用户需要配置$ chown $(id -u):$(id -g) $HOME/.kube/config # root用户需要配置$ echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; /etc/profile $ source /etc/profile Pod网络设置(flannel网络设置)12345678910111213141516171819$ mkdir -p /etc/cni/net.d/$ cat &lt;&lt;EOF&gt; /etc/cni/net.d/10-flannel.conf&#123; &quot;name&quot;: &quot;cbr0&quot;, &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: &#123; &quot;isDefaultGateway&quot;: true &#125;&#125;EOF $ mkdir /run/flannel/$ cat &lt;&lt;EOF&gt; /run/flannel/subnet.envFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.1.0/24FLANNEL_MTU=1450FLANNEL_IPMASQ=trueEOF# 添加网络类型$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml 检查master是否创建成功1234567891011# ready全部为1/1表示成功$ kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-78fcdf6894-cvrzg 1/1 Running 0 3h # 1/1表示正常kube-system coredns-78fcdf6894-sc2zd 1/1 Running 0 3h # 1/1表示正常kube-system etcd-k8s-master-1 1/1 Running 0 3hkube-system kube-apiserver-k8s-master-1 1/1 Running 0 3hkube-system kube-controller-manager-k8s-master-1 1/1 Running 0 3hkube-system kube-flannel-ds-2dzz9 1/1 Running 0 3h # 1/1表示正常kube-system kube-proxy-zgbcp 1/1 Running 0 3hkube-system kube-scheduler-k8s-master-1 1/1 Running 0 3h 其他命令12345678# 查看所有pod信息,需要使用--all-namespaces,不然默认参数是default$ kubectl get pod --all-namespaces# 查看节点信息$ kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master-1 Ready master 1d v1.11.1k8s-node-1 Ready &lt;none&gt; 2h v1.11.1k8s-node-2 Ready &lt;none&gt; 1d v1.11.1 参考使用 kubeadm 搭建 kubernetes 1.10.2 集群利用docker hub做中转拉取google的k8s镜像深入玩转K8S之使用kubeadm安装Kubernetes v1.10以及常见问题解答]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes参考资料.md]]></title>
    <url>%2Fblog%2F2018%2F08%2F08%2Fkubernetes%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[官方文档选择 Browse Docs Kubernetes中文社区Api ExampleBasic CommondKubernetes中文指南Istio其他什么是pause容器?]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes参考资料</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes之kubeadm安装.md]]></title>
    <url>%2Fblog%2F2018%2F08%2F08%2FKubernetes%E4%B9%8Bkubeadm%E5%AE%89%E8%A3%85-md%2F</url>
    <content type="text"><![CDATA[环境: Centos7, Kubernetes 1.11.2, docker-ce 17.03 ForewardKubernetes安装文档. 这里是一些安装软件的 kubeadm: the command to bootstrap the cluster. kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers. kubectl: the command line util to talk to your cluster. InstallPrepare添加yum仓库配置文件官方推荐的使用的packages.cloud.google.com地址不通,这里使用阿里云的yum仓库. 1234567891011121314151617181920$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 关闭SELinux12345678910# 临时关闭$ setenforce 0# 保证下次机器重启时生效.$ vim /etc/selinux/configSELINUX=disabled 网络设置12345678910$ vim cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl --system 关闭系统swap12$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 安装docker此处忽略,查看docker官方安装教程即可. Install Master1234$ yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes$ systemctl enable kubelet &amp;&amp; systemctl start kubelet]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubeadm安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blackbox_exporter之安装]]></title>
    <url>%2Fblog%2F2018%2F08%2F06%2FBlackbox-exporter%E4%B9%8B%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境:centos7, blackbox_exporter 0.12.0 前言这里使用blackbox_exporter的ssh和ping的检测功能. Build with Binary And Config SystemdBuild with Binary Download123$ cd /opt$ wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.12.0/blackbox_exporter-0.12.0.linux-amd64.tar.gz$ tar -xvf blackbox_exporter-0.12.0.linux-amd64.tar.gz &amp;&amp; mv blackbox_exporter-0.12.0.linux-amd64 blackbox_exporter-0.12.0 Config blackbox.yml12345678910111213$ vim /opt/blackbox_exporter-0.12.0/blackbox.ymlmodules: ssh_banner: prober: tcp timeout: 15s tcp: query_response: - expect: &quot;^SSH-2.0-&quot; icmp: prober: icmp timeout: 5s icmp: preferred_ip_protocol: &quot;ip4&quot; Config Systemd123456789101112131415161718$ vim /usr/lib/systemd/system/blackbox.service[Unit]Description=blackbox.serviceWants=network-online.targetAfter=network-online.target[Service]Type=simpleExecStart=/opt/blackbox_exporter-0.12.0/blackbox_exporter --config.file=/opt/blackbox_exporter-0.12.0/blackbox.ymlRestart=on-failureExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPID[Install]WantedBy=multi-user.target# 启动$ systemctl enable blackbox &amp;&amp; systemctl start blackbox Test blackbox在浏览器中查看:http://10.1.1.26:9115 Build with Docker]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>blackbox_exporter安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[altermanager之初体验]]></title>
    <url>%2Fblog%2F2018%2F08%2F05%2Faltermanager%E4%B9%8B%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[环境:centos7, alertmanager 0.15.1 github 前言刚入门alertmanager,权当记录. Execute AlertmanagerBuild with Binary and Config SystemdBuild Binary Download123456# 安装在/opt下.下载地址prometheus官网有提供.$ cd /opt$ wget https://github.com/prometheus/alertmanager/releases/download/v0.15.1/alertmanager-0.15.1.linux-amd64.tar.gz$ tar -xvf alertmanager-0.15.1.linux-amd64.tar.gz# 重命名$ mv alertmanager-0.15.1.linux-amd64 alertmanager-0.15.1 Config Systemd1234567891011121314151617$ vim /usr/lib/systemd/system/alertmanager.service[Unit]Description=alertmanager.serviceWants=network-online.targetAfter=network-online.target[Service]Type=simpleExecStart=/opt/alertmanager-0.15.1/alertmanager --config.file=/opt/alertmanager-0.15.1/alertmanager.ymlRestart=on-failureExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPID[Install]WantedBy=multi-user.target# 启动$ systemctl enable alertmanager &amp;&amp; systemctl start alertmanager Build with DockerConfiguration alertmanager.yml example123456789101112131415161718192021222324252627282930# $ vim /opt/alertmanager-0.15.1/alertmanager.ymlglobal: resolve_timeout: 5m smtp_smarthost: &apos;smtp.mxhichina.com:25&apos; smtp_from: &apos;xiaoxiangyoupin@basestonedata.com&apos; smtp_auth_username: &apos;xiaoxiangyoupin@basestonedata.com&apos; smtp_auth_password: &apos;Xiaoxiangyoupin1&apos;templates: - &apos;/tmp/alert_test.txt&apos; # 发送消息模版.route: group_by: [&apos;proxy&apos;] group_wait: 10s group_interval: 20s repeat_interval: 5m # 警告发送成功后,等待该配置时间后才再次发送. receiver: &apos;proxy-team&apos;# 以上配置会被routes标签继承或覆盖.# routes: receivers: # 接收方集合- name: &apos;proxy-team&apos; email_configs: - to: &apos;363054731@qq.com,jiangwe@basestonedata.com&apos; text: &apos;报警&apos;# 防止过度报警.#inhibit_rules:# - source_match:# severity: &apos;critical&apos;# target_match:# severity: &apos;warning&apos;# equal: [&apos;alertname&apos;, &apos;dev&apos;, &apos;instance&apos;]]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>alertmanager之初体验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus之监测服务器心跳]]></title>
    <url>%2Fblog%2F2018%2F08%2F04%2Fprometheus%E4%B9%8B%E7%9B%91%E6%B5%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BF%83%E8%B7%B3%2F</url>
    <content type="text"><![CDATA[环境:centos7, prometheus 2.3.1, blackbox_export 0.12.0 其他exporter 前言blackbox能支持什么协议:HTTP, HTTPS (via the http prober), DNS, TCP socket and ICMP.对于机器可用性的监测,最简单的莫过于使用ping或者ssh的方式.这里使用blackbox_exporter,基于使用node_exporter的经验,可能会认为需要将blackbox_exporter部署到每个节点下,而实际上通过一个部署了blackbox_exporter的节点去检测其他机器. Execute blackbox_exporterBuild with Binary and Config SystemdBuild Binary123456# prometheus官网有下载地址$ cd /opt$ wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.12.0/blackbox_exporter-0.12.0.linux-amd64.tar.gz$ tar -xvf blackbox_exporter-0.12.0.linux-amd64.tar.gz# 重命名$ mv blackbox_exporter-0.12.0.linux-amd64 blackbox_exporter-0.12.0 Config Systemd1234567891011121314151617$ vim /usr/lib/systemd/system/blackbox.service[Unit]Description=blackbox.serviceWants=network-online.targetAfter=network-online.target[Service]Type=simpleExecStart=/opt/blackbox_exporter-0.12.0/blackbox_exporter --config.file=/opt/blackbox_exporter-0.12.0/blackbox.ymlRestart=on-failureExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPID[Install]WantedBy=multi-user.target# 启动$ systemctl enable blackbox &amp;&amp; systemctl start blackbox Build with Docker123# pwd指定宿主机配置文件路径,我一般都放在/data/blackbox_exporter/config目录下$ docker build -t blackbox_exporter .$ docker run -d -p 9115:9115 --name blackbox_exporter -v `pwd`:/config blackbox_exporter --config.file=/config/blackbox.yml Configuration blackbox_exporter example.yml123456789101112131415# 这里暂时只使用ping和ssh验证主机有效性的方式,其他配置可参看github上的example.yml$ vim blackbox.yml# probe探测频率由prometheus中的scripe_timeout决定.modules: ssh_banner: prober: tcp timeout: 10s # 每次探测的网络超时时间,默认为10秒 tcp: query_response: - expect: &quot;^SSH-2.0-&quot; icmp: prober: icmp timeout: 10s icmp: preferred_ip_protocol: &quot;ip4&quot; Test Blackbox Exporter12345678# 测试目标10.1.1.21的ping信息$ curl http://10.1.1.26:9115/probe\?module\=icmp\&amp;target\=10.1.1.25......probe_success 0 # 目标主机ping失败# 测试目标10.1.1.21 22的ssh信息$ curl http://10.1.1.26:9115/probe\?module\=ssh_banner\&amp;target\=10.1.1.21:22......probe_success 1 # 目标主机ssh通过 参考blackbox_exporter github用Prometheus进行网络质量ping]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>prometheus服务器心跳监测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus配置zookeeper服务发现机制]]></title>
    <url>%2Fblog%2F2018%2F08%2F03%2Fprometheus%E9%85%8D%E7%BD%AEzookeeper%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[环境:zookeeper 3.4.12, prometheus 2.3.1, centos7.3,node_exporter 0.16.0 前言prometheus监听时,一般时配置到prometheus.yml中.机器数量少还好,如果机器数量大,每次改动再重启,简直就是个噩梦.还好它支持服务发现.如何文件、consul、zookeeper等.具体可以查看官网配置说明. 实践Prepare服务器及软件安装服务器 安装软件10.1.1.25 prometheus,zookeeper10.1.1.2110.1.1.2210.1.1.23 node_exporter安装过程不在累述,自行百度即可. Config Prometheus.yml现在我需要监听118.10.2.34服务器中的3台虚拟机10.1.1.21:9100,10.1.1.22:9100,10.1.1.23:9100zookeeper中的路径设置如下:/proxy/118.10.2.34:22/10.1.1.21:9100/proxy/118.10.2.34:22/10.1.1.22:9100/proxy/118.10.2.34:22/10.1.1.23:9100 promethues.yml123456789101112131415161718192021scrape_configs: - job_name: &apos;proxy&apos; honor_labels: true serverset_sd_configs: - servers: - &apos;10.1.1.25:2181&apos; paths: - &apos;/proxy&apos; relabel_configs: - source_labels: [&apos;__meta_serverset_path&apos;] # 待抓取的服务器集合路径 regex: &apos;^/proxy/([^/]+)/([^/]+)&apos; # 路径匹配规则 target_label: &apos;meta_ip&apos; replacement: &apos;$&#123;1&#125;&amp;$&#123;2&#125;&apos; - source_labels: [&apos;meta_ip&apos;] regex: &apos;(\d+.\d+.\d+.\d+):(\d+)&amp;(\d+.\d+.\d+.\d+):(\d+)&apos; # 解析出ip target_label: &apos;__address__&apos; replacement: &apos;$&#123;1&#125;:$&#123;2&#125;&apos; # 设置监听的服务器ip - source_labels: [&apos;meta_ip&apos;] regex: &apos;(\d+.\d+.\d+.\d+):(\d+)&amp;(\d+.\d+.\d+.\d+):(\d+)&apos; target_label: &apos;machine&apos; # 设置所属服务器,用于标识作用,便于以后分组 replacement: &apos;$&#123;3&#125;:$&#123;4&#125;&apos; 特别注意zookeeper中的节点,如10.1.1.26:9100其值必须设置为json格式,可以简单设置为{},否则prometheus不能解析到该节点. 效果检查 参考Prometheus ConfigurationZooKeeper serverset discovery Issues附录DownloadPrometheus/Node_exporterZookeeper]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>prometheus服务发现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus之入门]]></title>
    <url>%2Fblog%2F2018%2F08%2F02%2Fprometheus%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[环境:prometheus 2.3.1, blackbox_exporter 0.12.0, alertmanager 0.15.1, centos 7 前言应用场景:监测500+主机是否存活(通过ping或ssh的方式检测),并报警. 注意点 Prometheus即是一个CPU密集型（查询）也是一个IO密集型（数据落地）的，CPU数量是多多益善，内存越大越好（来缓存抓取的数据，所以应该减少不必要的业务数据导出），尽量要使用SSD（这个很关键！），因为一旦Prometheus的内存使用量达到阈值会停止抓取数据！这个停止抓取的时间，至少是分钟级，甚至是无法恢复！所以只要有条件就要用SSD。 Prometheus号称支持reload，但目测不是很好用，比如你修改了告警规则文件，重载之后，新旧告警规则似乎会一起计算执行…. Execute PrometheusBuild with Binary And ConfigBuild with Binary Download123456# prometheus官网有下载地址$ cd /opt$ wget https://github.com/prometheus/prometheus/releases/download/v2.3.2/prometheus-2.3.2.linux-amd64.tar.gz$ tar -xvf prometheus-2.3.2.linux-amd64.tar.gz# 重命名$ mv prometheus-2.3.2.linux-amd64 prometheus-2.3.2 Config prometheus.ymlblackbox_exporter和alertmanager的安装可以参见另外两篇文章《alertmanager之初体验》、《blackbox_exporter之安装》1234567891011121314151617181920212223242526272829303132333435363738394041# 由于机器数量较多,使用prometheus的服务发现,将所有机器配置到单独的文件中.$ vim /opt/machine.json[ &#123;&quot;targets&quot;:[&quot;122.227.184.61:20065&quot;],&quot;labels&quot;:&#123;&quot;machineName&quot;:&quot;guangzhou77d9&quot;&#125;&#125;,&#123;&quot;targets&quot;:[&quot;122.227.184.61:20063&quot;],&quot;labels&quot;:&#123;&quot;machineName&quot;:&quot;guangzhou77d8&quot;&#125;&#125;,...]# 配置prometheus.yml$ vim /opt/prometheus-2.3.1/prometheus.ymlglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: [&quot;10.1.1.26:9093&quot;] # alertmanager的地址.# Load rules once and periodically evaluate them according to the global &apos;evaluation_interval&apos;.rule_files: - &quot;/opt/proxy_rules.yml&quot; # 规则文件.# A scrape configuration containing exactly one endpoint to scrape:# Here it&apos;s Prometheus itself.scrape_configs: - job_name: &apos;machine_heart&apos; metrics_path: /probe params: module: [ssh_banner] file_sd_configs: - files: [&apos;/opt/machine.json&apos;] # 机器地址列表文件 refresh_interval: 5m relabel_configs: - source_labels: [address] regex: (.*) replacement: $&#123;1&#125; target_label: __param_target # 请求http://10.1.1.26:9155的请求参数target - source_labels: [__param_target] target_label: instance - target_label: address replacement: 10.1.1.26:9115 # blackbox_exporter的地址. Config rules.yml12345678910111213$ vim /opt/proxy_rules.ymlgroups: - name: machine_heart rules: - alert: &apos;ssh&apos; expr: probe_success&#123;job=&quot;machine_heart&quot;&#125; &lt; 1 for: 1m labels: severity: critical # team: &apos;proxy-team&apos; annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; down.&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; has been down for more than 1 minutes.&quot; Config Systemd123456789101112131415$ vim /usr/lib/systemd/system/prometheus.service[Unit]Description=prometheus.serviceWants=network-online.targetAfter=network-online.target[Service]Type=simpleExecStart=/opt/prometheus-2.3.1/prometheus --config.file=/opt/prometheus-2.3.1/prometheus.ymlRestart=on-failureExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPID[Install]WantedBy=multi-user.target# 启动$ systemctl enable prometheus &amp;&amp; systemctl start prometheus Test Prometheus参考基于Prometheus的分布式在线服务监控实践Prometheus 非官方中文手册使用Prometheus+grafana打造高逼格监控平台]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>prometheus入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化运维工具之OpsManage安装]]></title>
    <url>%2Fblog%2F2018%2F05%2F24%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7%E4%B9%8BOpsManage%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境: centos7,opsmanage v2.2.1 OpsManage github 前言安装使用]]></content>
      <categories>
        <category>自动化运维工具</category>
      </categories>
      <tags>
        <tag>自动化运维工具之OpsManage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构之定义]]></title>
    <url>%2Fblog%2F2018%2F05%2F02%2F%E6%9E%B6%E6%9E%84%E4%B9%8B%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[架构之定义前言之前嘴里常说架构师,但并没有去细究它的具体含义.偶然间看到别人分享了极客时间的《从0开始学架构》专栏.读了架构到底是什么这篇文章后,发现自己从来都没有思考过.(人最怕的就是不知道自己不知道)赶紧学习充电. 架构架构:软件系统的顶层结构.(引用文章的结论).没有知识体系,理解这些概念真的很难.没办法,只能尝试去理解,强行记忆了. 想要理解,需要几个概念进行比较理解. 系统与子系统 一些独立的“个体”,互相关联和协作.(个体可以是组件、子系统、模块) 模块与组件 模块是从逻辑角度理解,进行职责上的划分. 组件是从物理角度理解,便于复用. 框架 1、组件规范; 2、提供基础功能产品; 架构需要明确系统包含哪些“个体”,并且需要明确个体运作和协作的规则.]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch之基本命令]]></title>
    <url>%2Fblog%2F2018%2F04%2F24%2Felasticsearch%E4%B9%8B%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[基本命令 环境：Mac 10.13.4 、elasticsearch 5.5.1、jdk8 索引（Index）创建索引1234567891011121314151617181920212223242526$ put http://[ip]:[port]/[索引名]## 创建时指定分片（3个主分片，2个副本分片）参数:&#123; &quot;settings&quot; : &#123; &quot;index&quot; : &#123;&quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2&#125; &#125;&#125;## 修改副本分片$ put http://[ip]:[port]/[索引名]/_settings参数:&#123; &quot;number_of_replicas&quot; : 1&#125;&#125;## 创建索引，并添加log类型，字段为message，字段类型为string（已存在索引会失败）$ put http://[ip]:[port]/[索引名]参数：&#123; &quot;mappings&quot;: &#123; &quot;log&quot; : &#123; &quot;properties&quot; &#123; &quot;message&quot; :&#123;&quot;type&quot; : &quot;string&quot;&#125; &#125; &#125; &#125;&#125; 查询索引12345678910111213141516171819$ get http://[ip]:[port]/[索引名]/[_mapping,_settings,_aliase]&#123; &quot;[索引名]&quot;: &#123; &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123;&#125;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1524617001036&quot;, &quot;number_of_shards&quot;: &quot;3&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;jk7IHv-jQpyd4jwOeo86ag&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5050199&quot; &#125;, &quot;provided_name&quot;: &quot;[索引名]&quot; &#125; &#125; &#125;&#125; 删除索引12$ delete http://[ip]:[port]/[索引名],[索引名],...或者使用_all或*删除全部索引（注意：务必要在配置文件中禁用 action.destructive_requires_name:true） 修改索引修改索引副本数量12345$ put http://[ip]:[port]/[索引名]/_settings参数:&#123; &quot;number_of_replicas&quot; : 1&#125; 打开/关闭索引123456## 因为关闭索引磁盘空间并不会释放，造成磁盘空间浪费，因此一般禁用该功能，settings,cluster.indices.close.enable:false$ post http://[ip]:[port]/[索引名]/[_open,_close]返回值：&#123;&quot;acknowledged&quot;: true&#125; 映射修改字段12345678910111213141516## name为对象类型（Object datatype）$ put http://[ip]:[port]/[索引名]/_mappings/user参数：&#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;properties&quot;: &#123; &quot;last&quot;: &#123;&quot;type&quot; : &quot;string&quot;&#125; &#125; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 文档（Document）增删改查123456789101112131415161718192021222324252627282930313233343536373839404142434445## 添加文档$ put/post http://[ip]:[port]/[索引名]/[类型]/[_id] 参数：&#123;&quot;computer&quot;:&quot;huawei&quot;&#125;返回值：&#123;&quot;_index&quot;: &quot;index1_test&quot;,&quot;_type&quot;: &quot;test&quot;,&quot;_id&quot;: &quot;3&quot;,&quot;_version&quot;: 1,&quot;result&quot;: &quot;created&quot;,&quot;_shards&quot;: &#123; &quot;total&quot;: 2,&quot;successful&quot;: 1,&quot;failed&quot;: 0&#125;,&quot;created&quot;: true&#125;## 删除文档$ delete http://[ip]:[port]/[索引名]/[类型]/[_id]返回值：&#123;&quot;found&quot;: true,&quot;_index&quot;: &quot;index1_test&quot;,&quot;_type&quot;: &quot;test&quot;,&quot;_id&quot;: &quot;3&quot;,&quot;_version&quot;: 2,&quot;result&quot;: &quot;deleted&quot;,&quot;_shards&quot;: &#123; &quot;total&quot;: 2,&quot;successful&quot;: 1,&quot;failed&quot;: 0&#125;&#125;## 修改文档$ post http://[ip]:[port]/[索引名]/[类型]/[_id]/_update参数：&#123; &quot;doc&quot; : &#123; &quot;computer&quot; : &quot;apple&quot; &#125;&#125;## 查询文档$ get http://[ip]:[port]/[索引名]/[类型]/[_id]返回值：&#123;&quot;_index&quot;: &quot;index1_test&quot;,&quot;_type&quot;: &quot;test1&quot;,&quot;_id&quot;: &quot;100&quot;,&quot;_version&quot;: 2,&quot;found&quot;: true,&quot;_source&quot;: &#123; &quot;computer&quot;: &quot;apple&quot;&#125;&#125;真的如上面这么简单，就不是es了。=!=]]></content>
      <categories>
        <category>elaticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch基本命令</tag>
        <tag>es基本命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Swagger注解说明]]></title>
    <url>%2Fblog%2F2018%2F04%2F10%2FSwagger%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Swagger注解说明 环境：swagger 2.7.0 前言使用swagger也有一段时间了，对于注解还是很混乱，这里做个记录，方便后续查阅。 注解swagger的注解都在swagger-annotations中，当前版本总共27个注解。 123456&lt;dependency&gt; &lt;groupId&gt;io.swagger&lt;/groupId&gt; &lt;artifactId&gt;swagger-annotations&lt;/artifactId&gt; &lt;version&gt;1.5.13&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 现在逐一查看各个注解及使用。 @Api作用于class或interface上（推荐定义在接口上），说明该接口的作用。 12@Api(tags = &#123;&quot;数据接口&quot;&#125;, description = &quot;推荐系统&quot;)public interface DataCollectControllerApi &#123;&#125; 效果： @ApiOperation作用于方法上 1234@ApiOperation(value = &quot;查询用户行为&quot;, notes = &quot;查询用户行为数据&quot;)@PostMapping(value = &quot;/behavior&quot;)@ResponseBody ResultBean&lt;PageInfo&gt; queryUserBehaviorRecord(@RequestBody UserBehaviorReqDTO userBehaviorReqDTO); 效果： @ApiImplicitParams和@ApiImplicitParam（不建议使用）作用于方法上，但是不建议使用（原因：This is the only way to define parameters when using Servlets or other non-JAX-RS* environments），建议使用@ApiParam 123456789@ApiOperation(value = &quot;查询用户&quot;, notes = &quot;查询用户信息&quot;) @ApiImplicitParams(&#123; @ApiImplicitParam(name = &quot;userId&quot;, value = &quot;用户id&quot;, paramType = &quot;query&quot;, dataType = &quot;int&quot;), @ApiImplicitParam(name = &quot;userName&quot;, value = &quot;用户名&quot;, paramType = &quot;query&quot;, dataType = &quot;string&quot;) &#125;) @PostMapping(value = &quot;/user&quot;) @ResponseBody ResultBean queryUserInfo(@RequestParam(&quot;userId&quot;) Integer userId, @RequestParam(&quot;userName&quot;) String userName); name：参数名称，即下图的Parameter列 value：参数描述，即下图的Description列 paramType：参数类型，这里支持5种query,body,path,form,header，这里常用query和body。 body表示该参数为json。 paramType：参数数据类型，目前只支持基本数据类型，int，float，long等 效果： @ApiParam作用于方法和参数上（只有一个参数时可以在方法上使用，但通常都会有多个参数，所以建议在参数上使用） 12345678@ApiOperation(value = &quot;查询用户&quot;, notes = &quot;查询用户信息&quot;)@PostMapping(value = &quot;/user&quot;)@ResponseBodyResultBean queryUserInfo( @ApiParam(name = &quot;userId&quot;, value = &quot;用户id&quot;, type = &quot;Integer&quot;) @RequestParam(&quot;userId&quot;) Integer userId, @ApiParam(name = &quot;userName&quot;, value = &quot;用户名称&quot;, type = &quot;String&quot;) @RequestParam(&quot;userName&quot;) String userName); name：参数名称，即下图的Parameter列 value：参数描述，即下图的Description列 type：参数数据类型，即下图的Data Type列。可以忽略，swagger会根据SpringMVC中的注解（RequestParam,RequestBody）自动匹配参数类型（Parameter Type列）和参数数据类型（Data Type列）。如下图，参数使用@RequestParam，则Parameter Type为query，Data Type为对应参数类型； 参数使用@RequestBody，则Parameter Type为body，Data Type为对应参数类型； 效果： @ApiResponses和@ApiResponse作用于方法上，一般用于描述错误的响应信息。@ApiResponses用户描述一组错误响应。@ApiResponse用户描述具体的错误响应。 12345678910111213141516@ApiOperation(value = &quot;查询用户&quot;, notes = &quot;查询用户信息&quot;)@PostMapping(value = &quot;/user&quot;)@ResponseBody@ApiResponses(&#123; @ApiResponse(code = 1001, message = &quot;用户参数为空&quot;,responseHeaders = &#123; @ResponseHeader(name = &quot;header-1&quot;, description = &quot;header-1描述&quot;, response = String.class), @ResponseHeader(name = &quot;header-2&quot;, description = &quot;header-2描述&quot;, response = Integer.class), @ResponseHeader(name = &quot;header-3&quot;, description = &quot;header-3描述&quot;, response = ResultBean.class) &#125;), @ApiResponse(code = 1002, message = &quot;用户不存在&quot;, response = ResultBean.class), &#125;)ResultBean queryUserInfo( @ApiParam(name = &quot;userId&quot;, value = &quot;用户id&quot;) @RequestBody Integer userId, @ApiParam(name = &quot;userName&quot;, value = &quot;用户名称&quot;) @RequestParam(&quot;userName&quot;) String userName); code：返回码，对应HTTP Status Code列 message：返回码描述，对应Reason列 response：返回值类型，对应Response Model列 responseHeaders：返回值头信息，使用@ResponseHeader对返回值头的描述 name：响应头名称 description：响应头描述 response：响应头返回值类型 使用@ApiResponse时，如果同时使用response属性和responseHeaders属性，responseHeaders会失效（已测试）。另外，使用responseHeaders时，在使用@ResponseHeader时，ResponseHeader中的属性response使用类类型无效（已测试），默认为string 效果： @ResponseHeaderApiKeyAuthDefinition ApiModel ApiModelProperty Authorization AuthorizationScope BasicAuthDefinition Contact Example ExampleProperty Extension ExtensionProperty ExternalDocs Info License OAuth2Definition Scope SecurityDefinition SwaggerDefinition Tage 常用模版参考 https://my.oschina.net/zzuqiang/blog/793606 swagger注解类使用说明 http://www.cnblogs.com/softidea/p/6251249.html Swagger使用]]></content>
      <categories>
        <category>swagger</category>
      </categories>
      <tags>
        <tag>swagger注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark之IDEA创建Scala工程]]></title>
    <url>%2Fblog%2F2018%2F04%2F08%2FScala%E4%B9%8BIDEA%E5%88%9B%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Spark之IDEA创建Scala工程 环境：Mac 10.13.3，Intellij Idea 2016.3.5，scala 2.12.5 前言通过IDEA快速创建scala-maven工程，开发spark应用。 快速开始创建通过maven模板创建scala 后面就是创建目录的指定，这里省略。 注：由于这个模板比较老，所以要对pom做一些修改。 修改pom文件这里主要是修改scala的版本（默认的是2.7.0的，太老了），以及maven-scala-plugin版本。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.bsd.scala&lt;/groupId&gt; &lt;artifactId&gt;scala-003&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.14&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.specs&lt;/groupId&gt; &lt;artifactId&gt;specs&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 以上基本的一个scala项目就好了。（另外再删除掉test下的相关类即可） 参考 http://cwiki.apachecn.org/display/Spark/Index Spark翻译组]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>IDEA创建Scala工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql之常用操作]]></title>
    <url>%2Fblog%2F2018%2F04%2F04%2FMysql%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Mysql之常用操作 环境：mysql 5.7.14 前言这里记录mysql的一些操作用作应用。 用户相关修改用户名密法1$ /usr/local/mysql/bin/mysqladmin -u root password 创建用户并授权12$ CREATE USER &apos;username&apos;@&apos;%&apos; IDENTIFIED BY &apos;password&apos;;$ GRANT all privileges ON databasename.* TO &apos;username&apos;@&apos;%&apos; with grant options 字符设置场景一：utf8字符集数据库需要修改为utf8mb4 首先，先查询当前mysql是否支持utf8mb4，如果支持，修改my.cnf，重启mysql，再修改表或者表字段字符集。 12345## 配置my.cnf[client]default-character-set=utf8mb4 [mysql]default-character-set=utf8mb4 查询字符集123456789101112131415161718## 查看数据库支持字符集$ show variables like&apos;%char%&apos;; character_set_client utf8mb4 //客户端字符集 character_set_connection utf8mb4 //链接字符集 character_set_database utf8mb4 //数据库字符集 character_set_filesystem binary character_set_results utf8mb4 //结果字符集 character_set_server utf8mb4 //服务器字符集 character_set_system utf8mb4 //系统字符集 character_sets_dir /usr/local/mysql/share/charsets/## 查看mysql支持的字符集$ show charset;## 查看当前数据库编码：$ SHOW CREATE DATABASE db_name;## 查看表编码：$ SHOW CREATE TABLE tbl_name;## 查看字段编码：$ SHOW FULL COLUMNS FROM tbl_name; 修改字符集123456## 修改数据库字符¥ ALTER DATABASE [database_name] CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;## 修改表字符集$ ALTER TABLE [table_name] CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;## 修改表字段字符集$ ALTER TABLE [table_name] MODIFY COLUMN [column] varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; Tip:为了实现客户端utf8连接到MySQL后，使用的也是utf8mb4字符集，就在 mysqld配置中配置了 init_connect=’SET NAMES utf8mb4’ 表示初始化连接都设置为utf8mb4字符集，再配置一个 skip-character-set-client-handshake = true 忽略客户端字符集设置，不论客户端是何种字符集，都按照init_connect中的设置进行使用，这样就满足了应用的需求 12345678910## 配置my.cnf[client]default-character-set=utf8mb4 [mysql]default-character-set=utf8mb4[mysqld]character-set-server = utf8mb4collation-server = utf8mb4_unicode_ciinit_connect = &apos;SET NAMES utf8mb4&apos;character-set-client-handshake = true]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>Mysql常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swagger2问题汇总]]></title>
    <url>%2Fblog%2F2018%2F04%2F03%2Fswagger2%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[swagger2问题汇总 环境：swagger 2.7.0，spring-boot 1.5.9 前言这里是对在使用swagger时碰到的相关问题汇总。 踩坑Failed to start bean ‘documentationPluginsBootstrapper’; nested exception is java.lang.NullPointerExceptionswagger的配置如下： 12345678910111213141516171819@Configuration@EnableSwagger2public class Swagger2 &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(new ApiInfoBuilder() .title(&quot;测试&quot;) .description(&quot;测试&quot;) .termsOfServiceUrl(&quot;&quot;) .contact(&quot;jilingjun&quot;) .version(&quot;1.0&quot;) .build()) .select() .apis(RequestHandlerSelectors.basePackage(&quot;com.bsd.recommend.controller&quot;)) .paths(PathSelectors.any()) .build(); &#125;&#125; 详细错误如下： 123456789101112131415161718192021222324252627282930313233org.springframework.context.ApplicationContextException: Failed to start bean &apos;documentationPluginsBootstrapper&apos;; nested exception is java.lang.NullPointerException at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:176) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:112) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:879) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.finishRefresh(EmbeddedWebApplicationContext.java:144) ~[spring-boot-1.5.1.RELEASE.jar:1.5.1.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:545) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122) ~[spring-boot-1.5.1.RELEASE.jar:1.5.1.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:737) ~[spring-boot-1.5.1.RELEASE.jar:1.5.1.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:370) ~[spring-boot-1.5.1.RELEASE.jar:1.5.1.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:314) ~[spring-boot-1.5.1.RELEASE.jar:1.5.1.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1162) ~[spring-boot-1.5.1.RELEASE.jar:1.5.1.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1151) ~[spring-boot-1.5.1.RELEASE.jar:1.5.1.RELEASE] at com.bsd.recommend.RecommendApplication.main(RecommendApplication.java:23) ~[classes/:na]Caused by: java.lang.NullPointerException: null at springfox.documentation.builders.RequestHandlerSelectors$4.apply(RequestHandlerSelectors.java:97) ~[springfox-core-2.7.0.jar:2.7.0] at springfox.documentation.builders.RequestHandlerSelectors$4.apply(RequestHandlerSelectors.java:94) ~[springfox-core-2.7.0.jar:2.7.0] at com.google.common.base.Present.transform(Present.java:79) ~[guava-19.0.jar:na] at springfox.documentation.builders.RequestHandlerSelectors$5.apply(RequestHandlerSelectors.java:113) ~[springfox-core-2.7.0.jar:2.7.0] at springfox.documentation.builders.RequestHandlerSelectors$5.apply(RequestHandlerSelectors.java:110) ~[springfox-core-2.7.0.jar:2.7.0] at com.google.common.base.Predicates$AndPredicate.apply(Predicates.java:374) ~[guava-19.0.jar:na] at com.google.common.base.Predicates$AndPredicate.apply(Predicates.java:374) ~[guava-19.0.jar:na] at com.google.common.collect.Iterators$7.computeNext(Iterators.java:675) ~[guava-19.0.jar:na] at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) ~[guava-19.0.jar:na] at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138) ~[guava-19.0.jar:na] at springfox.documentation.spring.web.scanners.ApiListingReferenceScanner.scan(ApiListingReferenceScanner.java:48) ~[springfox-spring-web-2.7.0.jar:2.7.0] at springfox.documentation.spring.web.scanners.ApiDocumentationScanner.scan(ApiDocumentationScanner.java:67) ~[springfox-spring-web-2.7.0.jar:2.7.0] at springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper.scanDocumentation(DocumentationPluginsBootstrapper.java:95) ~[springfox-spring-web-2.7.0.jar:2.7.0] at springfox.documentation.spring.web.plugins.DocumentationPluginsBootstrapper.start(DocumentationPluginsBootstrapper.java:154) ~[springfox-spring-web-2.7.0.jar:2.7.0] at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173) ~[spring-context-4.3.6.RELEASE.jar:4.3.6.RELEASE] ... 14 common frames omitted 初步诊断，swagger在做scan操作抛出了空指针异常。现在只能google了，发现这事一个bug，并且在2.8.0之后进行了修复（2.8.0修复的问题在这里：https://github.com/springfox/springfox/issues/2186）。 （可参考：https://github.com/springfox/springfox/issues/1876） 这个问题是发生在对controller做了切面后才会出现，If I use AOP advice on my method within @RestController I face this issue too because Spring creates proxy class to wrap controller’s method call。 解决方案 swagger 2.7.0，spring-boot 1.5.1 既然是个bug，就应该有临时解决方案吧。这还真让我找到了，具体解决的源url就找不到了，就直接帖代码了。 123456789101112131415161718192021222324252627282930313233@Configuration@EnableSwagger2public class Swagger2 extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addRedirectViewController(&quot;/documentation/v2/api-docs&quot;, &quot;/v2/api-docs?group=restful-api&quot;); registry.addRedirectViewController(&quot;/documentation/swagger-resources/configuration/ui&quot;, &quot;/swagger-resources/configuration/ui&quot;); registry.addRedirectViewController(&quot;/documentation/swagger-resources/configuration/security&quot;, &quot;/swagger-resources/configuration/security&quot;); registry.addRedirectViewController(&quot;/documentation/swagger-resources&quot;, &quot;/swagger-resources&quot;); &#125; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry. addResourceHandler(&quot;/documentation/swagger-ui.html**&quot;).addResourceLocations(&quot;classpath:/META-INF/resources/swagger-ui.html&quot;); registry. addResourceHandler(&quot;/documentation/webjars/**&quot;).addResourceLocations(&quot;classpath:/META-INF/resources/webjars/&quot;); &#125; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2).apiInfo(apiInfo()).select() .apis(RequestHandlerSelectors.any()).paths(PathSelectors.any()) .build().forCodeGeneration(true); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder().title(&quot;测试&quot;) .description(&quot;测试&quot;) .termsOfServiceUrl(&quot;&quot;).contact(&quot;联系我&quot;).version(&quot;1.0&quot;).build(); &#125;&#125; 参考 https://github.com/springfox/springfox/issues/1876 NullPointerException with Spring Data Rest integration https://github.com/springfox/springfox/issues/1860 [spring boot]@EnableAspectJAutoProxy cause endpoint scanning failed]]></content>
      <categories>
        <category>swagger</category>
      </categories>
      <tags>
        <tag>swagger问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis之集群搭建]]></title>
    <url>%2Fblog%2F2018%2F04%2F02%2FRedis%E4%B9%8B%E9%9B%86%E7%BE%A4%E5%88%9B%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Redis之集群搭建 环境：Centos7，redis3.2 前言### Cluster添加密码前提：先创建无密码集群 方式一：手动添加密码到配置文件修改所有redis服务器的配置文件添加密码，然后关闭redis-cluster，然后重启redis-cluster 12requirepass &quot;abc&quot;masterauth &quot;abc&quot; 方式二：通过命令添加密码通过命令行设置密码 123456789## 登录有添加密码$ redis-cli -h [host] -p [port] -c$ config set masterauth abc$ config set requirepass abc$ config rewrite$ shutdown## 每个节点都执行上述操作，然后正常关闭即可。查看每个节点的conf配置文件，会发现都自动添加了requirepass &quot;abc&quot;masterauth &quot;abc&quot; 后续登录都需要添加密码 1234$ redis-cli -h [host] -p [port] -c -a [password]或者$ redis-cli -h [host] -p [port] -c$ auth [password] 参考 ​]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis集群</tag>
        <tag>redis集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里开源之Jarslink初体验]]></title>
    <url>%2Fblog%2F2018%2F03%2F31%2F%E9%98%BF%E9%87%8C%E5%BC%80%E6%BA%90%E4%B9%8BJarslink%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Jarslink初体验 环境：spring-boot 1.5.3，Jarslink 1.5.0.20171107，jdk8 前言对于一个新东西，通常用哲学的问题进行初步了解。它是什么？它从哪里来？能够解决什么问题（要到哪里去）？ 它是什么？它从那里来？ 阿里开源的，名为Jarslink并基于Java的模块化开发框架。它提供在运行时动态加载模块（jar包）、卸载、模块间调用的API。 能够解决什么问题？ Hello World新建一个Hello World工程模块]]></content>
      <tags>
        <tag>Jarslink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之安装]]></title>
    <url>%2Fblog%2F2018%2F03%2F29%2Fdocker%E4%B9%8B%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Docker之安装 环境：Centos6.5，Centos7前言1、EPEL（ Extra Packages for Enterprise Linux，企业版Linux的额外软件包)是yum的一个软件源，里面包含了许多基本源里没有的软件；2、docker要求服务CentOS6以上，kernel 版本必须2.6.32-431或更高；查看系统的版本和内核1234## 查看系统版本$ lsb_release -a## 查看内核版本$ uname -r Centos6.5安装docker要将Docker安装到CentOS上，要使用EPEL软件库，下载epel安装包并安装12$ wget http://ftp.jaist.ac.jp/pub/Linux/Fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm$ rpm -ivh epel-release-6-8.noarch.rpm 检查EPEL源1234567891011121314$yum repolistLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.skyshe.cn * epel: mirrors.opencas.cn * extras: mirrors.skyshe.cn * updates: centos.ustc.edu.cnrepo id repo name statusbase CentOS-6 - Base 6,575epel Extra Packages for Enterprise Linux 6 - x86_64 12,234extras CentOS-6 - Extras 62updates CentOS-6 - Updates 1,580repolist: 20,451EPEL已经在repo列出，并显示提供12234个软件包，EPEL源的配置安装到了/etc/yum.repos.d/epel.repo 安装123$ yum install -y docker-io$ service docker start$ chkconfig docker on 检查docker是否安装成功12345678910111213141516171819202122232425262728$docker infoContainers: 0Images: 0Storage Driver: devicemapper Pool Name: docker-8:3-276002-pool Pool Blocksize: 65.54 kB Backing Filesystem: extfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 305.7 MB Data Space Total: 107.4 GB Data Space Available: 4.873 GB Metadata Space Used: 729.1 kB Metadata Space Total: 2.147 GB Metadata Space Available: 2.147 GB Udev Sync Supported: true Deferred Removal Enabled: false Data loop file: /var/lib/docker/devicemapper/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata Library Version: 1.02.95-RHEL6 (2015-09-08)Execution Driver: native-0.2Logging Driver: json-fileKernel Version: 2.6.32-431.el6.x86_64Operating System: &lt;unknown&gt;CPUs: 1Total Memory: 996.4 MiBName: winter-01ID: KJ3G:XD6V:SDDV:5SGY:2TRQ:4XXR:XBPV:VLHE:XHHZ:F425:7G2B:G5D5 Centos7安装dockercentos7可以直接使用yum安装，不需要再添加其他软件源信息. 安装1$ yum install -y docker 添加镜像源使用daocloud的镜像加速器，在https://www.daocloud.io上注册账号，选择加速器 正常情况下，直接执行上图的命令即可，但为了安全起见，执行后，查看/etc/docker/daemon.json，是否格式正确。 1234567$ vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [ &quot;http://5c00508c.m.daocloud.io&quot; ], &quot;insecure-registries&quot;: []&#125; 验证123456## 启动$ service docker start## 配置开机启动$ chkconfig docker on## 下载镜像测试## docker pull hello-world Q&amp;A：Centos6.5安装好epel后，执行命令yum repolist报错，信息如下：1234$ yum repolistLoaded plugins: product-id, security, subscription-managerUpdating certificate-based repositories.Error: Cannot retrieve metalink for repository: epel. Please verify its path and try again 解决办法：vim /etc/yum.repos.d/epel.repo编辑[epel]下的baseurl前的#号去掉，mirrorlist前添加#号。正确配置如下：12345678[epel]name=Extra Packages for Enterprise Linux 6 - $basearchbaseurl=http://download.fedoraproject.org/pub/epel/6/$basearch#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&amp;arch=$basearchfailovermethod=priorityenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 执行1$yum makecache 参考 https://www.jianshu.com/p/3a4cd73e3272 CentOS7安装Docker https://blog.csdn.net/Mr_OOO/article/details/67016309 Docker国内镜像源设置]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础之Collection]]></title>
    <url>%2Fblog%2F2018%2F03%2F29%2FJava%E5%9F%BA%E7%A1%80%E4%B9%8BCollection%2F</url>
    <content type="text"><![CDATA[java集合之HashMap 环境：jdk8 前言hashmap是非常常用的，深入了解还是很有好处的。对于hashmap或其他东西，了解一些关键和核心就能抓住要点，深入太多的其他细节，反而分散注意力，性价比反而不高。 对于HashMap，其关键点有这些：底层的数据结构是是什么？存储值的如何取值的？hash碰撞了怎么办？线程不安全会引发的问题？ hashmap在jdk7和jdk8中实现是不一样的。 jdk7中，hashmap底层的数据结构数组+单链表 jdk8中，hashmap底层的数据结构数组+单链表+红黑树 围绕以上的问题，开始对它进行剖析。 深入HashMapHashmap类图 参考 https://tech.meituan.com/java-hashmap.html Java 8系列之重新认识HashMap]]></content>
      <categories>
        <category>java集合</category>
      </categories>
      <tags>
        <tag>java集合</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo进阶之添加评论]]></title>
    <url>%2Fblog%2F2018%2F03%2F28%2Fhexo%E8%BF%9B%E9%98%B6%E4%B9%8B%E6%B7%BB%E5%8A%A0%E8%AF%84%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[Hexo进阶之添加评论 工具：hexo 3.6.0 + github pages + git + NexT 5.1.4 环境：mac(10.13.2)、node.js(9.4.0)、npm(5.6.0) 前言hexo可以使用的第三方插件有这几种： Gitment 需要登录github账号，不提供游客评论。 来必力（推荐） 支持多账号登录评论 Disqus Valine 多说，已不支持 网易云跟帖，已不支持 使用来必力注册liverslivere有两个版本： City 版：是一款适合所有人使用的免费版本 Premium 版：是一款能够帮助企业实现自动化管理的多功能收费版本 使用City版就行了。 获取安装代码 填写完成后，进入到 管理页面 -&gt; 代码管理 -&gt; 一般网站 代码中，data-uid即为next主题所需要的livere_uid。 修改主题配置123$ vim theme/next/_configlivere_uid: data-uid## 本地无需重启，要部署到github pages需要重新部署 使用Gitment参考 http://www.zhoujy.me/2017/07/16/livere/ 为Hexo主题yilia添加livere(来必力)评论支持 http://www.hl10502.com/2017/03/24/hexo-config-livere/ hexo添加LiveRe评论支持 本文作者：ttbb本文地址： http://steven-ji.github.io/blog/2018/03/28/hexo进阶之添加评论/ 版权声明：本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 CN 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo添加评论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo进阶之局部优化]]></title>
    <url>%2Fblog%2F2018%2F03%2F27%2Fhexo%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%B1%80%E9%83%A8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Hexo进阶之局部优化字数统计、Fork me置头像、打赏、社交链接前言这篇主要是一些局部优化，添加字数统计、添加Fork me、添加头像 添加字数统计 安装wordcount插件 12345$ npm install hexo-wordcount --save主要功能字数统计:WordCount阅读时长预计:Min2Read总字数统计: TotalCount 编辑主题配置，开启功能 12345$ vim theme/next/layout/_layout.swigpost_wordcount: item_text: true wordcount: true min2read: true 添加Fork me on GitHub 去网址https://github.com/blog/273-github-ribbons 挑选自己喜欢的样式，并复制代码，添加到themes\next\layout_layout.swig的body标签之内即可。把里面的url换成自己的! 设置头像修改主题配置_config.yml中的avatar字段，添加头像路径。 两种添加方式：一设置头像url地址；二是上传头像图片，设置图片路径； 我采用第二种，在source下新建uploads目录，图片上传即可 1avatar: /uploads/avatar.png 添加社交链接修改主题配置_config.yml中的social字段 123$ vim theme/next/_config.ymlsocial: GitHub: https://github.com/steven-ji || github 站点建立时间 修改主题配置文件，修改字段since 1since: 2013 打赏 修改主题配置文件 123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /blog/uploads/wechatpay.pngalipay: /blog/uploads/alipay.png 注：如果访问的博客有带根路径，如我的博客地址是https://steven-ji.github.io/blog/，则需要在图片路径前加/blog，不然访问不到图片。 文末添加版权声明12345$ npm install hexo-addlink --save## 修改站点配置文件,addlink支持markdown语法addlink: before_text: __本文作者__：ttbb&lt;br /&gt;__本文地址__： after_text: &lt;br /&gt;__版权声明__：本博客所有文章除特别声明外，均采用 [CC BY-NC-SA 3.0 CN](http://creativecommons.org/licenses/by-nc-sa/3.0/cn/) 许可协议。转载请注明出处！ 文章分享朋友圈功能(不支持https)使用第三方插件jiathis 注册账号注册成功后，获取uid。（在设置-&gt;基本设置-&gt;账户信息中） 修改jiathis.swig文件如果不需要的分享地方可以删除了 1234567891011$ vim themes\next\layout\_partials\share\jiathis.swig&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;&lt;a class=&quot;jiathis_button_fav&quot;&gt;收藏夹&lt;/a&gt;&lt;a class=&quot;jiathis_button_copy&quot;&gt;复制网址&lt;/a&gt;&lt;a class=&quot;jiathis_button_email&quot;&gt;邮件&lt;/a&gt;&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;&lt;a class=&quot;jiathis_button_qzone&quot;&gt;QQ空间&lt;/a&gt;&lt;a class=&quot;jiathis_button_tqq&quot;&gt;腾讯微博&lt;/a&gt;&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;&lt;a class=&quot;jiathis_button_share&quot;&gt;一键分享&lt;/a&gt; 开启服务修改主题配置文件，使用注册后获得的uid 12345$vim theme/next/_config.ymljiathis: enable: true# Warning: JiaThis does not support https. add_this_id: yourUID 配置网站图标修改主题配置文件 1234## 上图网站图标到theme/next/source/images/目录下,配置网站图标名称$ vim theme/next/_config.ymlfavicon: medium: /images/favicon.png 参考 http://theme-next.iissnan.com/theme-settings.html 主题配置 http://barrysite.me/2017/05/07/Hexo%20Next%E4%B8%BB%E9%A2%98%E4%B8%8B%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/ Hexo Next主题下基本配置 http://zhy.one/html/hexo-addlink.html 用hexo-addlink在文章尾部插入当前文章链接 本文作者：ttbb本文地址： http://steven-ji.github.io/blog/2018/03/27/hexo进阶之局部优化/ 版权声明：本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 CN 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo局部优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo进阶之添加分类、标签、搜索菜单]]></title>
    <url>%2Fblog%2F2018%2F03%2F26%2Fhexo%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%88%86%E7%B1%BB%E3%80%81%E6%A0%87%E7%AD%BE%E3%80%81%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[hexo进阶之添加菜单分类、标签、搜索 工具：hexo + NexT 环境：mac(10.13.2)、node.js(9.4.0)、npm(5.6.0) 前言上一篇已经搭建了最简单的博客，没有常见的菜单：分类、标签、音乐、搜索、RSS订阅、个人，这一篇就是完善这个菜单功能。 添加菜单 修改NexT下的主题配置文件12345678910111213141516$ vim theme/next/_config.yml## 添加菜单(配置菜单对应的图标,NexT使用的是Font Awesome提供的图标https://fontawesome.com/)## || 后面为图标名称menu: home: / || home categories: /categories/ || th archives: /archives/ || archive tags: /tags/ || tags #search: /search/ || search ## 不需要配置，使用了local search #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap about: /about/ || user #commonweal: /404/ || heartbeat# Enable/Disable menu icons.menu_icons: enable: true 编辑页面显示的菜单中文名称编辑Next目录下的languages/{language}.yml文件，{language}.yml为站点_config.yml中配置的languages: zh-Hans对应 1234567891011$ vim /theme/next/languages/zh-Hans.ymlmenu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益404 到这里，已经添加了归档、分类等菜单，但是点击是没有效果的。 配置菜单对应内容分类菜单1234567## 创建categories页面$ hexo new page &quot;categories&quot;$ vim source/categories/index.md title: All categories date: 2018-03-26 12:39:04 type: &quot;categories&quot; comments: false 标签菜单1234567## 创建tags页面$ hexo new page &quot;tags&quot;$ vim source/tags/index.md title: All tags date: 2014-12-22 12:39:04 type: &quot;tags&quot; comments: false 注：分类和标签还没有和分类关联起来，需要在每篇文章头部添加categories和tags属性，这样就可以自动关联了，为了后续方便，修改scaffolds/post.md模版即可。 123456title: hexo+github快速搭建博客date: 2018-03-25 14:12:32categories: hexotags: - hexo+github+next- hexo 搜索菜单采用Hexo提供的Local Search站内搜索,原理是通过hexo-generator-search插件在本地生成一个search.xml文件，搜索的时候从这个文件中根据关键字检索出相应的链接。 123456789101112## 在站点根目录下安装$ npm install hexo-generator-search --save$ npm install hexo-generator-searchdb --save## 站点_config.yml配置search: path: search.xml field: post format: html limit: 10000## 主题配置文件修改local_search: enable: true 注：由于使用了local search，主题配置文件中就menu属性就不需要添加search了。 参考 https://theme-next.iissnan.com/getting-started.html NexT开始使用 https://github.com/iissnan/hexo-theme-next/blob/master/README.cn.md NexT使用文档 http://cherryblog.site/Hexo-high-level-tutorialcloudmusic,bg-customthemes-statistical.html hexo搭建 本文作者：ttbb本文地址： http://steven-ji.github.io/blog/2018/03/26/hexo进阶之分类、标签、搜索/ 版权声明：本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 CN 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo+github快速搭建博客]]></title>
    <url>%2Fblog%2F2018%2F03%2F25%2Fhexo%2Bgithub%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[自建博客最佳实践本篇是自己实践记录。 工具：hexo 3.6.0 + github pages + git + NexT 5.1.4 环境：mac(10.13.2)、node.js(9.4.0)、npm(5.6.0) 前言开始之前，先回答三个哲学问题，它是谁？它从哪里来？它要到哪里去？ hexo A fast, simple &amp; powerful blog framework, powered by Node.js. 一个github的开源项目。hexojs/hexo 说白了，就是本来想要搭建一个博客（比较麻烦的是页面样式等），而别人已经给你搞了个框架，你只管创作，其他的都不要你操心（给你一个漂亮的页面），交给它就可以了，给你生成一个静态的页面，放哪都能访问。 GitHub Pages Github Pages 本身就是 Github 为用户提供的一个介绍项目、发表技术文章的网站。当你创建了它，这时可以认为你已经拥有了一个网站，只不过这个网站没有内容。Github Pages 的网站内容和样式全部可以由用户自己定制，网站空间无限，流量免费，Github为你维护，安全又稳定，而且你可以很轻松的更新它。Github Pages 实际上是一个 Github 仓库，这也就解释了为什么其很容易更新，因为网站的样式和内容都存储在该仓库中，当你更新仓库时，网站也自动更新了。 说白了，就是一个免费的服务器，把hexo生成的静态页面往上一放，哪哪都能访问了。 准备环境安装node.js 方式一 123456789101112131415161718192021##查看node版本，建议安装最新版本9.4.0$ node -vv4.4.0##清除node.js的cache$ sudo npm cache clean -f 注：这里可能会报错，如果报错，放弃使用这种方式，使用方式二 npm WARN using --force I sure hope you know what you are doing. npm ERR! code MODULE_NOT_FOUND npm ERR! Cannot find module &apos;internal/util/types&apos; npm ERR! A complete log of this run can be found in: npm ERR! /var/root/.npm/_logs/2017-11-20T02_16_00_192Z-debug.log##安装 n 工具，这个工具是专门用来管理node.js版本的$ sudo npm install -g n##安装指定版本的node.js（稳定版|最新版|9.4.0版本），也可以删除sudo n rm 9.0.0$ sudo n stable|latest|9.4.0注：安装过程可能会失败，再次安装会报dyld: bad external relocation length的错误，这是由于残留未下载完的文件，删除即可。路径：/usr/local/n/versions/node##再次检查版本$ node -vv9.4.0$ sudo npm install npm@latest -g 方式二（推荐使用） 1$ brew install node 安装git1$ brew install git git的相关配置就不再累述，google一把把 安装hexo1234567891011121314## 一行搞定$ npm install hexo-cli -g ##当出现这种错误时， Cannot find module &apos;../lib/utils/unsupported.js&apos; 采用下面方式，重新安装node $ rm -rf /usr/local/lib/node_modules # remove previously installed node $ brew uninstall node $ brew prune # clean all broken symlinks $ brew update # always good to have the latest # and install normal one $ brew install node $ hexo -v 配置GitHub Pages 新建博客仓库 这里就建一个名为blog的仓库，初始化README勾选上。 开启gh-pages功能 点击Settings 找到GitHub Pages，选择使用master分支。 保存后，就算发布成功了，访问链接地址试试吧。 配置hexo并部署启动hexo12345678910## 新建一个文件夹用于存放框架，如blog$ mkdir -p /usr/xxxx/Documents/blog$ cd /usr/xxxx/Documents/blog$ hexo init $ npm install## 启动$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.##本地就能查看效果 将博客与Github关联 1.编辑主站_config.yml(即/usr/xxxx/Documents/blog目录下) 12345678910## 配置访问地址和访问根路径url: http://**.github.io/blog/ 这地址在开启github pages时生成root: /blog注：如果我的创建的仓库为testblog，则url和root相应的修改为http://**.github.io/testblog/和/testblog## 配置部署路径deploy: type: git repository: git@github.com:***/blog branch: master 部署到GitHub12345678910## 安装部署插件$ npm install hexo-deployer-git --save## 清除缓存$ hexo clean## 本地生成静态文件$ hexo g## 部署$ hexo d## 访问https://***.github.io/blog/ 到此，大功告成！ 这里还有点小瑕疵，默认主题我不是很喜欢，这里使用next 使用NexT主题1234567891011121314151617181920212223242526## 到hexo root目录下$ cd hexo$ ls_config.yml node_modules package.json public scaffolds source themes$ mkdir themes/next$ git clone https://github.com/iissnan/hexo-theme-next themes/next$ cd themes/next$ git tag -l…v5.1.0v5.1.1v5.1.2v5.1.3v5.1.4$ git checkout tags/v5.1.0Note: checking out &apos;tags/v5.1.0&apos;.…HEAD now on 1f72f68... CSS: Remove global list-style setting of ul## 配置主站_config.yml（不是theme/next下的_config.yml）language: zh-Hanstheme: next## 重新发布即可$ hexo clean$ hexo g$ hexo d 现在算是完成了！ 后续进阶 如何使用自己的域名？ 分类、标签、音乐、搜索、RSS订阅，怎么做？ 如何统计访问量？ 如何开通留言功能？ 如何提供大赏功能？ 如何提高博客搜索排名（SEO）？ 参考 使用 Hexo + Github Pages 搭建独立博客 http://yanshengjia.com/2017/01/31/%E4%BD%BF%E7%94%A8Hexo-Github-Pages%E6%90%AD%E5%BB%BA%E7%8B%AC%E7%AB%8B%E5%8D%9A%E5%AE%A2/ 我用hexo部署到github-pages上的网页没有样式 http://www.coin163.com/it/x4042410272215986758/hexo-github-pages 附录 hexo官网 https://hexo.io/zh-cn/，建站详细信息可参考 NexT https://github.com/iissnan/hexo-theme-next ，主题 Q&amp;A部署到github-pages上的网页没有样式这种情况时漏配置了站点_config.yml中的url和root导致 12url: http://**.github.io/blog/root: /blog hexo生成的文档，不能正确解析语法中的标题这是在hexo3.1之后，对语法校验更严格了，#后面需要添加空格 本文作者：ttbb本文地址： http://steven-ji.github.io/blog/2018/03/25/hexo+github快速搭建博客/ 版权声明：本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 CN 许可协议。转载请注明出处！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo+github+next</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
